{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2533568",
   "metadata": {
    "id": "b2533568"
   },
   "source": [
    "\n",
    "# Intro to Neural Networks for Regression\n",
    "Course: ITCS 4156 - Introduction to Machine Learning \n",
    "\n",
    "Instructor: Xiang Zhang\n",
    "\n",
    "$\\newcommand{\\xv}{\\mathbf{x}}\n",
    " \\newcommand{\\wv}{\\mathbf{w}}\n",
    " \\newcommand{\\yv}{\\mathbf{y}}\n",
    " \\newcommand{\\zv}{\\mathbf{z}}\n",
    " \\newcommand{\\uv}{\\mathbf{u}}\n",
    " \\newcommand{\\vv}{\\mathbf{v}}\n",
    " \\newcommand{\\tv}{\\mathbf{t}}\n",
    " \\newcommand{\\bv}{\\mathbf{b}}\n",
    " \\newcommand{\\av}{\\mathbf{a}}\n",
    " \\newcommand{\\Chi}{\\mathcal{X}}\n",
    " \\newcommand{\\R}{\\rm I\\!R}\n",
    " \\newcommand{\\sign}{\\text{sign}}\n",
    " \\newcommand{\\Ym}{\\mathbf{Y}}\n",
    " \\newcommand{\\Xm}{\\mathbf{X}}\n",
    " \\newcommand{\\Wm}{\\mathbf{W}}\n",
    " \\newcommand{\\Zm}{\\mathbf{Z}}\n",
    " \\newcommand{\\Im}{\\mathbf{I}}\n",
    " \\newcommand{\\Um}{\\mathbf{U}}\n",
    " \\newcommand{\\Vm}{\\mathbf{V}}\n",
    " \\newcommand{\\Am}{\\mathbf{A}}\n",
    " \\newcommand{\\muv}{\\boldsymbol\\mu}\n",
    " \\newcommand{\\Sigmav}{\\boldsymbol\\Sigma}\n",
    " \\newcommand{\\Lambdav}{\\boldsymbol\\Lambda}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba8af07",
   "metadata": {
    "id": "fba8af07"
   },
   "source": [
    "<br/>\n",
    "<font color=\"blue\"><b>\n",
    "\n",
    "NAME: *Nick Koehler*\n",
    "\n",
    "</b> </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3b523d",
   "metadata": {
    "id": "1f3b523d"
   },
   "source": [
    "## Goal \n",
    "The goal of this lab will be to practice implementing the feed-forward and feedback processes used by neural networks. We'll also take a look at manual hyper-parameter tuning. We'll do so by, once again, working with the Forest Fire dataset as we have already performed data exploration and preparation so that we can get to implementing neural networks more quickly.\n",
    "\n",
    "Your job is to read through the lab and fill in any code segments that are marked by `TODO` headers and comments. **It should be noted, that all the correct outputs are given below each code cell. It might be useful to duplicate all the `TODO` cells so you can try to match the correct output with your own code!**\n",
    "\n",
    "Use the `todo_check()`to help guide you in understanding whether your code for a given TODO is correct or incorrect. However, failing a TODO check doesn't mean you won't receive points, though it could be a good indication. If you are failing feel free to ask and we can help check what is happening.\n",
    "\n",
    "## Agenda\n",
    "- Review and load the Forest Fires dataset\n",
    "- Visualize and explore the Forest Fires dataset\n",
    "- Create the data preparation pipeline where we apply data preprocessing AFTER splitting\n",
    "- Implement a simple 2 layer neural network\n",
    "    - Implement identity/linear, sigmoid, and tanh activation functions.\n",
    "    - Implement the neural network feed-forward process for making predictions\n",
    "    - Implement the neural network feedback process for updating the weights and biases\n",
    "- Investigate model tuning by manually turning hyper-parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44daf24",
   "metadata": {
    "id": "a44daf24"
   },
   "source": [
    "## Table of notation\n",
    "\n",
    "| Symbol                     | Meaning                     | Symbol    | Meaning                                                          |\n",
    "|----------------------------|-----------------------------|-----------|------------------------------------------------------------------|\n",
    "| $\\xv$ or $\\vec{x}$         | feature/input vector        | $x_i$     | $i$th element of $\\xv$                                           |\n",
    "| $\\Xm$                      | input matrix                | $x_{i,j}$ | $i$th row and $j$th column of $\\Xm$                              |\n",
    "| $\\yv$ or $\\tv$             | labels/targets              | $n$       | number of features or columns \n",
    "| $\\wv$ or $\\mathbf{\\theta}$ | weight/parameter vector     | $m$       | number of data samples <br>(also used to refer to the slope) |samples or rows                                   |\n",
    "| $f$ or $h$                 | hypothesis function <br> (i.e., a model)        | $\\hat{\\yv}$ <br> $f(\\xv {;} \\wv)$<br>$h(\\xv {;} \\wv)$ | predictions <br> y-hat |\n",
    "| $E$              | error or sum of error (loss)  | $SSE$      | sum of squared error function                                            |\n",
    "| $MSE$                      | mean squared error| $\\nabla$  | gradient (nabla)                                       |\n",
    "| $\\partial$                 | partial derivative          | $\\alpha$  | learning rate (alpha)                                  |       \n",
    "| $J$ | general placeholder for <br>the objective function | $x^T$| transpose of a vector or matrix |\n",
    "$b$ | bias or y-intercept term | $T$ | Threshold |\n",
    "$*$| element-wise<br> multiplication | $\\cdot$ | dot product|\n",
    "| $z$<br>$\\zv$| value before applying activation function |  $X, Y$ | Random variables |\n",
    "| $K$| number/set of classes | $k$ | current class|\n",
    "| $MLE$|  maximum likelihood estimation | $ML$ |  maximum likelihood|\n",
    "| $MLL$|  maximum log likelihood | $LL$ | log likelihood |\n",
    "| $L$|  likelihood | $NLL$ | negative log likelihood |\n",
    "| $g$ | activation function | $a$/$h$ <br> $\\av/\\mathbf{h}$<br>$\\Am$/$\\mathbf{H}$ | output of activation function <br> or neuron\n",
    "$w$<br>$\\wv$<br>$\\Wm$ | weights| $z$<br>$\\zv$<br>$\\Zm$ | linear combination output|\n",
    "|$\\Wm^{[l]}$| $l$th layer weights| $\\Am^{[l]}$| $l$th layer activations\n",
    "|$\\Zm^{[l]}$| $l$th layer linear combinations| $\\bv^{[l]}$| $l$th layer bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfaed5b",
   "metadata": {
    "id": "3cfaed5b"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f6cf537",
   "metadata": {
    "id": "7f6cf537"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import gc\n",
    "from typing import Tuple, Union, List, Dict\n",
    "\n",
    "import sklearn\n",
    "sklearn_version = '1.0'\n",
    "# Check to make sure you have the right version of sklearn\n",
    "assert sklearn.__version__  > sklearn_version, f'sklearn version is only {sklearn.__version__} and needs to be > {sklearn_version}'\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "\n",
    "np.set_printoptions(suppress=True) \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e578ddee",
   "metadata": {
    "id": "e578ddee"
   },
   "outputs": [],
   "source": [
    "# Set this to True if you DO NOT want to run the \n",
    "# garbage_collect() functions throughout the notebook\n",
    "turn_off_garbage_collect = False\n",
    "\n",
    "def garbage_collect(vars_):\n",
    "    if not turn_off_garbage_collect:\n",
    "        for v in vars_:\n",
    "            if v in globals():\n",
    "                del globals()[v]\n",
    "        collected = gc.collect()\n",
    "\n",
    "def todo_check(condi_err):\n",
    "    failed_err = \"You passed {}/{} and FAILED the following code checks:{}\"\n",
    "    failed = \"\"\n",
    "    n_failed = 0\n",
    "    for check, (condi, err) in enumerate(condi_err):\n",
    "        if not condi:\n",
    "            n_failed += 1\n",
    "            failed += f\"\\nFailed check [{check+1}]:\\n\\t Tip: {err}\"\n",
    "\n",
    "    if len(failed) != 0:\n",
    "        passed = len(condi_err) - n_failed\n",
    "        err = failed_err.format(passed, len(condi_err), failed)\n",
    "        raise AssertionError(err.format(failed))\n",
    "    print(\"Your code PASSED the code check!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806fd51f",
   "metadata": {
    "id": "806fd51f"
   },
   "source": [
    "# Problem statement summary and data loading\n",
    "\n",
    "![](https://fee.org/media/38322/forest-fire-3782544_1280.jpg?anchor=center&mode=crop&height=656&widthratio=2.1341463414634146341463414634&rnd=132451547710000000)\n",
    "\n",
    "### Problem statement summary\n",
    "\n",
    "Recall that our goal here is to help prevent and make firefighting easier by identifying \"high risk\" areas by predicting how much area of a forest could burn when a fire occurs. We are attempting to achieve this by using supervised learning where our labels corresponds to the amount of forest area (in hectares) that has burned in the past."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b969dac8",
   "metadata": {
    "id": "b969dac8"
   },
   "source": [
    "### Data loading\n",
    "\n",
    "To begin, let's fetch the Forest Fire dataset. If you don't have it then download it using the following link: https://archive.ics.uci.edu/ml/datasets/Forest+Fires. When you arrive at the UCI webite click the \"Data Folder\" button near the top right to download the dataset. Once clicked, download the following file: `forestfires.csv`.\n",
    "\n",
    "Be sure to move the `forestfires.csv` file to the **SAME** directory/folder that this Jupyter Notebook is in (i.e., the current path of the notebook). We need to do this so that when we go to load the data we can easily path to the files! Run the below code to check which will print your notebook's current path and which directory the notebook is in!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ed8c23c",
   "metadata": {
    "id": "2ed8c23c",
    "outputId": "be87c657-84e9-4c7a-e56d-e9952a2ce2bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current path for your notebook is:\n",
      " C:\\Users\\leapi\\Documents\\School\\2022-2023\\Spring\\MachineLearning\\pastAsignments\n",
      "\n",
      "Your notebook is currently in the following directory:\n",
      " pastAsignments\n"
     ]
    }
   ],
   "source": [
    "print(f\"The current path for your notebook is:\\n {os.getcwd()}\\n\")\n",
    "print(f\"Your notebook is currently in the following directory:\\n {os.path.basename(os.getcwd())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30179ca",
   "metadata": {
    "id": "b30179ca"
   },
   "source": [
    "#### TODO 1\n",
    "Complete the TODO by loading the `forestfires.csv`.\n",
    "\n",
    "1. Load Forest Fires dataset using the Pandas. Store the output into the `forestfire_df` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f0b1167",
   "metadata": {
    "id": "0f0b1167",
    "outputId": "2eeee1fc-2aa2-4dc8-f373-44e380ae07d6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\leapi\\AppData\\Local\\Temp\\ipykernel_16168\\3156517158.py:4: UserWarning: The forestfires.csv is not detected in your local path! You need to move the 'forestfires.csv' file to the same location/directory as this notebook which is C:\\Users\\leapi\\Documents\\School\\2022-2023\\Spring\\MachineLearning\\pastAsignments\n",
      "  warnings.warn(f\"The forestfires.csv is not detected in your local path! \" \\\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'forestfires.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe forestfires.csv is not detected in your local path! \u001b[39m\u001b[38;5;124m\"\u001b[39m \\\n\u001b[0;32m      5\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou need to move the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mforestfires.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m file to the same \u001b[39m\u001b[38;5;124m\"\u001b[39m \\\n\u001b[0;32m      6\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocation/directory as this notebook which is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos\u001b[38;5;241m.\u001b[39mgetcwd()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)   \n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# TODO 1.1\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m forestfire_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforestfires.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m forestfire_df\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    665\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    666\u001b[0m     dialect,\n\u001b[0;32m    667\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    676\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    677\u001b[0m )\n\u001b[0;32m    678\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 680\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    572\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    574\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 575\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:933\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    930\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    932\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 933\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1217\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1213\u001b[0m     mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1214\u001b[0m \u001b[38;5;66;03m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[0;32m   1215\u001b[0m \u001b[38;5;66;03m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[0;32m   1216\u001b[0m \u001b[38;5;66;03m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[1;32m-> 1217\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[0;32m   1218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1224\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1226\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1227\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1228\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py:789\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    784\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    785\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    786\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    787\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    788\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    796\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    797\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    798\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'forestfires.csv'"
     ]
    }
   ],
   "source": [
    "# This line checks to make sure the forestfire.csv is in the \n",
    "# same directory as this notebook.\n",
    "if not os.path.exists(\"forestfires.csv\"):\n",
    "    warnings.warn(f\"The forestfires.csv is not detected in your local path! \" \\\n",
    "                    f\"You need to move the 'forestfires.csv' file to the same \" \\\n",
    "                    f\"location/directory as this notebook which is {os.getcwd()}\")   \n",
    "# TODO 1.1\n",
    "forestfire_df = pd.read_csv(\"forestfires.csv\")\n",
    "forestfire_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1c7831",
   "metadata": {
    "id": "3e1c7831"
   },
   "source": [
    "# Visualization and exploration\n",
    "\n",
    "Now, we have already done most of the visualization and data exploration in prior labs. However, let's start by reviewing some of the ideas we learned while exploring the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db24bef1",
   "metadata": {
    "id": "db24bef1"
   },
   "source": [
    "## Observing linear and non-linear trends\n",
    "\n",
    "First, let's review how each one of our features correlates with our target 'area'. Recall, we have take the log of our target 'area' in order to spread out the skewed distribution towards being more normal. Below, is the code from previous labs for plotting our target 'area' against every input feature. \n",
    "\n",
    "Once again, take note that none of our features, when compared 1-1 with our target, seem to show a linear or non-linear trend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f636a8",
   "metadata": {
    "id": "29f636a8",
    "outputId": "78f2d06f-4b04-4e18-dba5-24c445e1c660"
   },
   "outputs": [],
   "source": [
    "log_area_values = np.log1p(forestfire_df['area'])\n",
    "features = forestfire_df.drop('area', axis=1)\n",
    "\n",
    "fig, _ = plt.subplots(figsize=(15, 13))\n",
    "\n",
    "# column_name holds the current column name\n",
    "# idx holds the current column index\n",
    "# You can use either to index features to get the current feature\n",
    "for idx, column_name in enumerate(features.columns.values):\n",
    "    plt.subplot(3, 4, idx+1)\n",
    "    plt.plot(features[column_name], log_area_values, '.')\n",
    "    plt.ylabel('Area')\n",
    "    plt.xlabel(column_name)\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66511b6",
   "metadata": {
    "id": "e66511b6"
   },
   "source": [
    "# Data Preparation Pipeline\n",
    "\n",
    "Now it's time to recreate our data preparation pipelines we used in prior weeks. In this lab we will, once again, apply all our data cleaning and transformation operations AFTER splitting the data since the data is technically partly split already. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07136df",
   "metadata": {
    "id": "e07136df"
   },
   "source": [
    "## Dropping outliers\n",
    "Recall, before we do anything, we need to remove any extreme outliers that might distort the performance of our model. Below we define a function for you for dropping outliers that we used in prior labs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59da3fbe",
   "metadata": {
    "id": "59da3fbe"
   },
   "outputs": [],
   "source": [
    "from scipy.stats import zscore\n",
    "\n",
    "def outlier_locations(z, threshold):\n",
    "    abs_z = np.abs(z)\n",
    "    outlier_locs = np.where(abs_z > threshold)\n",
    "    return outlier_locs[0]\n",
    "\n",
    "def drop_outliers(df, threshold, verbose=False):\n",
    "    numerical_cols_df = df.drop(['day', 'month', 'area'], axis=1)\n",
    "    # Get z-scores\n",
    "    z = numerical_cols_df.apply(zscore)\n",
    "\n",
    "    # Find outlier locations\n",
    "    outlier_locs = outlier_locations(z, threshold=threshold)\n",
    "    \n",
    "    # Drop samples\n",
    "    new_df = df.drop(outlier_locs, axis=0)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Dropping...\")\n",
    "        print(f\"forestfire_df BEFORE dropping: {forestfire_df.shape}\")\n",
    "        print(f\"forestfire_df AFTER dropping: {new_df.shape}\")\n",
    "        display(df.iloc[outlier_locs])\n",
    "\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf8b570",
   "metadata": {
    "id": "aaf8b570"
   },
   "source": [
    "Below, the `TEST_drop_outliers()` function provides example code for running the `drop_outliers()` function. Notice, using a threshold of 5 removes any data sample whose feature values are greater than 5 standard deviations above or below the feature mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bc262a",
   "metadata": {
    "id": "08bc262a",
    "outputId": "fc275604-7ed3-4a2c-cdb5-7da532b49d78"
   },
   "outputs": [],
   "source": [
    "def TEST_drop_outliers():\n",
    "\n",
    "    dropped_forestfire_df = drop_outliers(\n",
    "        forestfire_df, \n",
    "        threshold=5, \n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "TEST_drop_outliers()\n",
    "garbage_collect(['TEST_drop_outliers'])    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ec5e39",
   "metadata": {
    "id": "d3ec5e39"
   },
   "source": [
    "## Splitting data\n",
    "\n",
    "Now onto splitting! Below we redefine the splitting functions `feature_label_split()` and `train_valid_test_split()` which we have seen in prior labs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218e7c62",
   "metadata": {
    "id": "218e7c62"
   },
   "outputs": [],
   "source": [
    "def feature_label_split(df: pd.DataFrame, \n",
    "                        label_name: str) -> Tuple[pd.DataFrame]:\n",
    "    \"\"\" Split dataframe into features and labels\n",
    "    \n",
    "        Args:\n",
    "            df: DataFrame containing both features and labels\n",
    "            \n",
    "            label_name: Name of the column which contains the labels\n",
    "    \"\"\"\n",
    "    \n",
    "    X = df.drop(label_name, axis=1)\n",
    "    y = df[[label_name]].copy()\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89826f3e",
   "metadata": {
    "id": "89826f3e"
   },
   "source": [
    "Below, within the `TEST_feature_label_split()` function is an example of how to call the `feature_label_split()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62ab902",
   "metadata": {
    "id": "d62ab902",
    "outputId": "eb441926-676a-465e-a646-7092b195facc"
   },
   "outputs": [],
   "source": [
    "def TEST_feature_label_split():\n",
    "    # Drop outliers\n",
    "    dropped_forestfire_df = drop_outliers(forestfire_df, threshold=5)\n",
    "    # Apply feature and label splitting\n",
    "    X, y = feature_label_split(dropped_forestfire_df, label_name='area')\n",
    "\n",
    "    print(f\"X shape: {X.shape}\")\n",
    "    print(f\"y shape: {y.shape}\")\n",
    "    \n",
    "TEST_feature_label_split()\n",
    "garbage_collect(['TEST_feature_label_split'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928e484c",
   "metadata": {
    "id": "928e484c"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def train_valid_test_split(\n",
    "    X: pd.DataFrame, \n",
    "    y: pd.DataFrame, \n",
    "    seed: int = 42\n",
    ") -> Tuple[pd.DataFrame]:\n",
    "    \"\"\" Split data into trining, validation, and test sets\n",
    "        \n",
    "        Args:\n",
    "            X: Features typically given as a Panda's DataFrame\n",
    "            \n",
    "            y: Targets/labels typically given as a Panda's DataFrame\n",
    "            \n",
    "            seed: Seed to create reproducible validation and testing splits\n",
    "    \"\"\"\n",
    "    X_trn, X_tst, y_trn, y_tst = train_test_split(X, y, test_size=.2, random_state=seed)\n",
    "    X_trn, X_vld, y_trn, y_vld = train_test_split(X_trn, y_trn, test_size=.2, random_state=seed)\n",
    "    \n",
    "    return X_trn, y_trn, X_vld, y_vld, X_tst, y_tst"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02dcfdb3",
   "metadata": {
    "id": "02dcfdb3"
   },
   "source": [
    "Below, within the `TEST_train_valid_test_split()` function is an example of how to call the `train_valid_test_split()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39025fc8",
   "metadata": {
    "id": "39025fc8",
    "outputId": "f59770a8-51bd-462e-f094-d4a17b11596a"
   },
   "outputs": [],
   "source": [
    "def TEST_train_valid_test_split():\n",
    "    \n",
    "    # Drop outliers\n",
    "    dropped_forestfire_df = drop_outliers(forestfire_df, threshold=5)\n",
    "    # Apply feature and label splitting\n",
    "    X, y = feature_label_split(dropped_forestfire_df, label_name='area')\n",
    "    # Apply train, validation and test set splitting\n",
    "    X_trn, y_trn, X_vld, y_vld, X_tst, y_tst = train_valid_test_split(X, y)\n",
    "\n",
    "    print(f\"X_trn shape: {X_trn.shape}\")\n",
    "    print(f\"y_trn shape: {y_trn.shape}\")\n",
    "    print(f\"X_vld shape: {X_vld.shape}\")\n",
    "    print(f\"y_vld shape: {y_vld.shape}\")\n",
    "    print(f\"X_tst shape: {X_tst.shape}\")\n",
    "    print(f\"y_tst shape: {y_tst.shape}\")\n",
    "\n",
    "TEST_train_valid_test_split()\n",
    "garbage_collect(['TEST_train_valid_test_split'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fffc2c2",
   "metadata": {
    "id": "6fffc2c2"
   },
   "source": [
    "## Transforming and cleaning data\n",
    "\n",
    "This module's data cleaning will contain nothing new. As we are working with the Forest Fire dataset we'll apply the `LogTransformer`class to the targets while applying `OneHotEncoding` and `Standardization` classes to the features. Keep in mind, we'll also be using Sklearn's `ColumnTransformer` and our own `DataFrameColumnTransformer` which applies Sklearn's `ColumnTransformer`  but returns the output as a DataFrame.\n",
    "\n",
    "If you get lost be sure to reference the regression labs from early in the semester for help!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771985a8",
   "metadata": {
    "id": "771985a8"
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2d51fe",
   "metadata": {
    "id": "6c2d51fe"
   },
   "source": [
    "### DataFrameColumnTransformer\n",
    "\n",
    "Below we redefine our `DataFrameColumnTransformer` which we'll need when conducting feature preprocessing. Remember, this class acts just like Sklearn's `ColumnTransformer` which will only apply certain preprocessing steps to certain columns in our DataFrame. The difference is that the output of our `DataFrameColumnTransformer` will be DataFrame and not a NumPy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbebedc",
   "metadata": {
    "id": "4dbebedc"
   },
   "outputs": [],
   "source": [
    "class DataFrameColumnTransformer(TransformerMixin):\n",
    "    def __init__(self, stages: List[Tuple]):\n",
    "        self.col_trans = ColumnTransformer(stages, remainder='passthrough')\n",
    "    \n",
    "    def fit(self, X: pd.DataFrame):\n",
    "        \"\"\" Runs our ColumnTransformer.fit() method \"\"\"\n",
    "        self.col_trans.fit(X)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\" Runs our ColumnTransformer.transform() method \"\"\"\n",
    "        output_arr = self.col_trans.transform(X)\n",
    "        \n",
    "        return self.to_dataframe(output_arr)\n",
    "    \n",
    "    def to_dataframe(self, arr: np.ndarray) -> pd.DataFrame:\n",
    "        \"\"\"Converts our output of ColumnTransformer into a DataFrame\"\"\"\n",
    "        feature_names = self.col_trans.get_feature_names_out()\n",
    "        \n",
    "        # Remove the \"__\" that ColumnTransformer adds to our feature names\n",
    "        # when we call self.col_trans.get_feature_names_out()\n",
    "        for i, name in enumerate(feature_names):\n",
    "            if '__' in name:\n",
    "                feature_names[i] = name.split('__', 1)[-1]\n",
    "        \n",
    "        # Creates a Pandas Dataframe\n",
    "        df = pd.DataFrame(arr, columns=feature_names)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a1a73e",
   "metadata": {
    "id": "22a1a73e"
   },
   "source": [
    "### Target cleaning\n",
    "\n",
    "The only preprocessing steps for our targets will be to apply the log transform using the `LogTransformer` class. Recall that we do this in an attempt to make the target values less of a skewed distribution by using the log transform to help spread out the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7352dc2c",
   "metadata": {
    "id": "7352dc2c"
   },
   "outputs": [],
   "source": [
    "class LogTransformer(BaseEstimator, TransformerMixin): \n",
    "    def __init__(self):\n",
    "        self.feature_names = None\n",
    "    \n",
    "    def fit(self, y: pd.DataFrame):\n",
    "        \n",
    "        # We don't need to set/learn any variables so\n",
    "        # we just need to return a reference to the object with 'self'\n",
    "        # If we dont return self the Pipeline class will throw errors\n",
    "        return self\n",
    "    \n",
    "    def transform(self, y: pd.DataFrame) -> pd.DataFrame:\n",
    "        self.feature_names = y.columns\n",
    "        \n",
    "        return np.log1p(y)\n",
    "    \n",
    "    def get_feature_names_out(self, name=None) -> pd.Series:\n",
    "        return self.feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ad1c44",
   "metadata": {
    "id": "60ad1c44"
   },
   "source": [
    "#### TODO 2\n",
    "\n",
    "Complete the TODO by coding the `target_pipeline()` function.\n",
    "\n",
    "1. Complete the `target_pipeline()` function as follows: \n",
    "    1. Define an instance of Sklearn's `Pipeline` class which applies the `LogTransformer` class. \n",
    "\n",
    "    2. Fit and transform the training labels `y_trn`.\n",
    "\n",
    "    3. Transform the validation labels `y_vld`. \n",
    "\n",
    "    4. Transform the test labels `y_tst`.\n",
    "\n",
    "    5.  Return the cleaned output for training, validation, and testing labels. The order of returned values should be training, validation, and then testing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fd0dec",
   "metadata": {
    "id": "b6fd0dec"
   },
   "outputs": [],
   "source": [
    "# TODO 2.1\n",
    "def target_pipeline(y_trn, y_vld, y_tst):\n",
    "    \"\"\" Creates Pipeline to apply data cleaning and transformations \n",
    "        to the targets/labels.\n",
    "        \n",
    "        Args:\n",
    "            y_trn: train labels\n",
    "            \n",
    "            y_vld: validation labels\n",
    "            \n",
    "            y_tst: test labels\n",
    "    \"\"\"\n",
    "    pipe = sklearn.pipeline.Pipeline([('logT', LogTransformer())])\n",
    "    y_trn = pipe.fit_transform(y_trn)\n",
    "    y_vld = pipe.transform(y_vld)\n",
    "    y_tst = pipe.transform(y_tst)\n",
    "    return y_trn, y_vld, y_tst\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bb25e4",
   "metadata": {
    "id": "88bb25e4"
   },
   "source": [
    "Run the below `TEST_target_pipeline()` function to test your implementation of the `target_pipeline()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59d03d2",
   "metadata": {
    "id": "b59d03d2",
    "outputId": "9174c36a-7de5-4077-a767-8f9f47a7c3c1"
   },
   "outputs": [],
   "source": [
    "def TEST_target_pipeline():\n",
    "    # Drop outliers\n",
    "    dropped_forestfire_df = drop_outliers(forestfire_df, threshold=5)\n",
    "    # Apply feature and label splitting\n",
    "    X, y = feature_label_split(dropped_forestfire_df, label_name='area')\n",
    "    # Apply train, validation and test set splitting\n",
    "    X_trn, y_trn, X_vld, y_vld, X_tst, y_tst = train_valid_test_split(X, y)\n",
    "    # Apply  data cleaning and transformations\n",
    "    y_trn, y_vld, y_tst = target_pipeline(y_trn, y_vld, y_tst)\n",
    "\n",
    "    print(f\"y_trn shape: {y_trn.shape}\")\n",
    "    print(f\"y_trn type: {type(y_trn)}\")\n",
    "    print(f\"y_vld shape: {y_vld.shape}\")\n",
    "    print(f\"y_vld type: {type(y_vld)}\")\n",
    "    print(f\"y_tst shape: {y_tst.shape}\")\n",
    "    print(f\"y_tst type: {type(y_tst)}\")\n",
    "\n",
    "    todo_check([\n",
    "        (np.all(np.isclose(y_trn.iloc[:3].values.reshape(-1,), [0, 1.854734, 2.833213], rtol=.01)),'y_trn possibly contains incorrect values'),\n",
    "        (np.all(np.isclose(y_vld.iloc[:3].values.reshape(-1,), [0, 2.489894, 1.435085], rtol=.01)), 'y_vld possibly contains incorrect values'),\n",
    "        (np.all(np.isclose(y_tst.iloc[:3].values.reshape(-1,), [0.343590, 0, 0], rtol=.01)), 'y_tst possibly contains incorrect values')\n",
    "    ])\n",
    "\n",
    "TEST_target_pipeline()\n",
    "garbage_collect(['TEST_target_pipeline'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea71ceb7",
   "metadata": {
    "id": "ea71ceb7"
   },
   "source": [
    "### Feature cleaning\n",
    "\n",
    "The only feature cleaning classes we'll need is our `OneHotEncoding` and `Standardization` classes which we redefine for you below. \n",
    "\n",
    "**Notice we have NO `AddBias` class!** Although the bias term is still required, for neural networks we'll treat the bias as a separate term which we can optimize for directly for the sake of clarity. This means we don't need to add a column of 1s to our data as, once again, the bias will be treated separately from the weights!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7c3ee0",
   "metadata": {
    "id": "de7c3ee0"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "class OneHotEncoding(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, feature_names='auto'):\n",
    "        self.feature_names = feature_names\n",
    "        self.encoder = OneHotEncoder(categories=feature_names, sparse=False)\n",
    "\n",
    "    def fit(self, X: pd.DataFrame):\n",
    "        \n",
    "        self.encoder.fit(X)\n",
    "        \n",
    "        # Store names of features\n",
    "        self.feature_names = self.encoder.get_feature_names_out()\n",
    "        return self\n",
    "\n",
    "    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        \n",
    "        one_hot =  self.encoder.transform(X)\n",
    "\n",
    "        return pd.DataFrame(one_hot, columns=self.get_feature_names_out())\n",
    "    \n",
    "    def get_feature_names_out(self, name=None)-> pd.Series:\n",
    "        return self.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56eac76",
   "metadata": {
    "id": "e56eac76"
   },
   "outputs": [],
   "source": [
    "class Standardization(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.feature_names = None\n",
    "    \n",
    "    def fit(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        \n",
    "        self.mean = np.mean(X, axis=0)\n",
    "        self.std = np.std(X, axis=0)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        self.feature_names = X.columns\n",
    "        return (X  - self.mean) / self.std\n",
    "\n",
    "    def get_feature_names_out(self, name=None) -> pd.Series:\n",
    "        return self.feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe84e666",
   "metadata": {
    "id": "fe84e666"
   },
   "source": [
    "#### TODO 3\n",
    "Complete the TODO by coding the `feature_pipeline()` function.\n",
    "\n",
    "1. Define the `feature_pipeline()` function as follows:\n",
    "    1. First, declare an instance of `DataFrameColumnTransformer` which applies the `OneHotEncoding()` class to the ONLY the \"day\" and \"month\" features. \n",
    "    \n",
    "    2. Now, declare an instance of the Sklearn's `Pipeline` class which is a list of tuples. The 1st tuple should apply your `DataFrameColumnTransformer`  instance and the 2nd tuple should apply an instance of the `Standardization()` class.\n",
    "    \n",
    "    3. Fit and transform the training data `X_trn` using your `Pipeline` class instance.\n",
    "\n",
    "    4. Transform the validation data `X_vld` using your `Pipeline` class instance.\n",
    "\n",
    "    5. Transform the test data `X_tst` using your `Pipeline` class instance.\n",
    "\n",
    "    6.  Return the cleaned output for training, validation, and testing data. The order of returned values should be training, validation, and then testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4042dce4",
   "metadata": {
    "id": "4042dce4"
   },
   "outputs": [],
   "source": [
    "# TODO 3.1\n",
    "def feature_pipeline(X_trn: pd.DataFrame, \n",
    "                     X_vld: pd.DataFrame, \n",
    "                     X_tst: pd.DataFrame) -> List[pd.DataFrame]:\n",
    "    \"\"\" Creates column transformers and pipelines to apply data clean and \n",
    "        transfornations to the input features of our data.\n",
    "        \n",
    "        Args:\n",
    "            X_trn: train features\n",
    "            \n",
    "            X_vld: validation features\n",
    "            \n",
    "            X_tst: test features\n",
    "    \"\"\"\n",
    "    OHencoder = DataFrameColumnTransformer([('oHot', OneHotEncoding(), ['day', 'month'])])\n",
    "    \n",
    "    pipe = Pipeline([('oHot', OHencoder),('standardization', Standardization())])\n",
    "    \n",
    "    X_trn = pipe.fit_transform(X_trn)\n",
    "    X_vld = pipe.transform(X_vld)\n",
    "    X_tst = pipe.transform(X_tst)\n",
    "    return X_trn, X_vld, X_tst"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f9aff7",
   "metadata": {
    "id": "b6f9aff7"
   },
   "source": [
    "Run the below `TEST_feature_pipeline()` function to check you implementation of the `feature_pipeline()` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f924753",
   "metadata": {
    "id": "0f924753",
    "outputId": "feb955ac-4144-4ca6-d607-ad7cf8c09574",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def TEST_feature_pipeline(): \n",
    "    \n",
    "    # Drop outliers\n",
    "    dropped_forestfire_df = drop_outliers(forestfire_df, threshold=5)\n",
    "    # Apply feature and label splitting\n",
    "    X, y = feature_label_split(dropped_forestfire_df, label_name='area')\n",
    "    # Apply train, validation and test set splitting\n",
    "    X_trn, y_trn, X_vld, y_vld, X_tst, y_tst = train_valid_test_split(X, y)\n",
    "\n",
    "    X_trn, X_vld, X_tst = feature_pipeline(X_trn, X_vld, X_tst)\n",
    "    print(X_tst.iloc[:3, 5].values)\n",
    "    print(f\"X_trn shape: {X_trn.shape}\")\n",
    "    print(f\"X_trn type: {type(X_trn)}\")\n",
    "    print(f\"X_vld shape: {X_vld.shape}\")\n",
    "    print(f\"X_vld type: {type(X_vld)}\")\n",
    "    print(f\"X_tst shape: {X_tst.shape}\")\n",
    "    print(f\"X_tst type: {type(X_tst)}\")\n",
    "    display(X_trn)\n",
    "\n",
    "    todo_check([\n",
    "        (X_trn.shape[1] == 29, \"X_trn does not have enough columns\"),\n",
    "        ('day_fri' in X_trn.columns, \"X_trn is missing one-hot encoded columns\"),\n",
    "        ('month_apr' in X_vld.columns, \"X_vld is missing one-hot encoded columns\"),\n",
    "        ('day_thu' in X_tst.columns, \"X_tst is missing one-hot encoded columns\"),\n",
    "        (np.all(np.isclose(X_trn.iloc[:3, 5], [-0.37862467,  2.64113797, -0.37862467], rtol=.01)), \"X_trn possibly contains incorrect values!\"),\n",
    "        (np.all(np.isclose(X_vld.iloc[:3, 5], [ 2.64113797, -0.37862467, -0.37862467], rtol=.01)), \"X_vld possibly contains incorrect values!\"),\n",
    "        (np.all(np.isclose(X_tst.iloc[:3, 5], [-0.37862467,  2.64113797, -0.37862467], rtol=.01)), \"X_tst possibly contains incorrect values!\"),\n",
    "    ])\n",
    "    \n",
    "TEST_feature_pipeline()\n",
    "garbage_collect(['TEST_feature_pipeline'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5db71c",
   "metadata": {
    "id": "7a5db71c"
   },
   "source": [
    "## Putting it all together\n",
    "\n",
    "In order to make our lives easier we created a function called `data_prep()` which will apply the outlier removal, data splitting, and data cleaning/transforming for us. Thus, whenever we go to implement a new algorithm we can call `data_prep()` to give us our data and override any other variables with the same names!\n",
    "\n",
    "**Take time to also read the DocStrings or in-line documentation, given at the start of the function, which describes what each argument does. You need to understand what each argument does before moving forward!**\n",
    "\n",
    "*Note: Any arguments in a function given after the `*` in the function definition below MUST be passed using the keyword. See this [post](https://stackoverflow.com/questions/14301967/bare-asterisk-in-function-arguments) for more information.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbc73d2",
   "metadata": {
    "id": "4bbc73d2"
   },
   "outputs": [],
   "source": [
    "def data_prep(df: pd.DataFrame,\n",
    "              label_name: str,\n",
    "              *,\n",
    "              seed: int = 42,\n",
    "              return_array: bool = False,\n",
    "              outlier_threshold: int = 5) -> Tuple[pd.DataFrame]:\n",
    "    \"\"\" Removes outliers, splits data and runs target/feature data cleaning \n",
    "        and transformations.\n",
    "    \n",
    "        Args:\n",
    "            df: A Pandas DataFrame containing our dataset for the \n",
    "                current lab.\n",
    "                \n",
    "            label_name: Name of the column in the DataFrame store in df \n",
    "                which will be used as the label/target. This will be \n",
    "                passed to the feature_label_split() function.\n",
    "            \n",
    "            seed: The seed used when splitting data into train, \n",
    "                validation, and test. This will be passed to the\n",
    "                train_valid_test_split() function.\n",
    "            \n",
    "            return_array: A boolean which when True will return all data as NumPy\n",
    "                arrays instead of Pandas DataFrames.\n",
    "            \n",
    "            outlier_threshold: outlier threshold where if a data sample exceeds\n",
    "                the STD threshold for any given feature then it is dropped. This\n",
    "                will be passed to the drop_outliers() function.\n",
    "    \"\"\"   \n",
    "    # Drop outliers\n",
    "    dropped_df = drop_outliers(df, threshold=outlier_threshold)\n",
    "    # Apply feature and label splitting\n",
    "    X, y = feature_label_split(dropped_df, label_name=label_name)\n",
    "    # Apply train, validation and test set splitting\n",
    "    X_trn, y_trn, X_vld, y_vld, X_tst, y_tst = train_valid_test_split(X, y, seed)\n",
    "    # Target cleaning\n",
    "    y_trn, y_vld, y_tst = target_pipeline(y_trn, y_vld, y_tst)\n",
    "    # Feature cleaning\n",
    "    X_trn, X_vld, X_tst = feature_pipeline(X_trn, X_vld, X_tst)\n",
    "    \n",
    "    # Resets the Pandas index for the Dataframe and series\n",
    "    # This will prevent any headaches when combining or indexing\n",
    "    # the train, validation, and test data in the future.\n",
    "    X_trn.reset_index(inplace=True, drop=True)\n",
    "    y_trn.reset_index(inplace=True, drop=True)\n",
    "    X_vld.reset_index(inplace=True, drop=True)\n",
    "    y_vld.reset_index(inplace=True, drop=True)\n",
    "    X_tst.reset_index(inplace=True, drop=True)\n",
    "    y_tst.reset_index(inplace=True, drop=True)\n",
    "        \n",
    "    # Return data as arrays instead of DataFrames\n",
    "    if return_array:\n",
    "        X_trn, y_trn, X_vld, y_vld, X_tst, y_tst = (X_trn.values, \n",
    "                                                    y_trn.values, \n",
    "                                                    X_vld.values, \n",
    "                                                    y_vld.values, \n",
    "                                                    X_tst.values, \n",
    "                                                    y_tst.values)\n",
    "                               \n",
    "    \n",
    "    return X_trn, y_trn, X_vld, y_vld, X_tst, y_tst"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a7ba8a",
   "metadata": {
    "id": "69a7ba8a"
   },
   "source": [
    "#### TODO 4\n",
    "Complete this TODO by calling `data_prep()` as specified below.\n",
    "\n",
    "\n",
    "1. Call the `data_prep()` so that it will return the Forest Fire regression data formatted as NumPy arrays by passing the arguments which correspond to the following:\n",
    "    1. Pass all required arguments for `df` and `label_name`.\n",
    "    1. Return all data as NumPy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2bab65",
   "metadata": {
    "id": "cc2bab65",
    "outputId": "c4b1a15d-456e-458c-81ed-2cb101d80daf"
   },
   "outputs": [],
   "source": [
    "def TEST_data_prep():\n",
    "    # TODO 4.1\n",
    "    data = data_prep(forestfire_df, 'area', return_array = True)\n",
    "    X_trn, y_trn, X_vld, y_vld, X_tst, y_tst = data\n",
    "\n",
    "    print(f\"X_trn type: {type(X_trn)}\")\n",
    "    print(f\"X_trn shape: {X_trn.shape}\")\n",
    "    print(f\"y_trn shape: {y_trn.shape}\")\n",
    "    print(f\"X_vld type: {type(X_vld)}\")\n",
    "    print(f\"X_vld shape: {X_vld.shape}\")\n",
    "    print(f\"y_vld shape: {y_vld.shape}\")\n",
    "    print(f\"X_tst type: {type(X_tst)}\")\n",
    "    print(f\"X_tst shape: {X_tst.shape}\")\n",
    "    print(f\"y_tst shape: {y_tst.shape}\")\n",
    "\n",
    "    todo_check([\n",
    "        (type(X_trn) is np.ndarray, \"X_trn is not a NumPy array\"),\n",
    "        (np.all(np.isclose(y_trn[:3].flatten(), np.array([0., 1.85473427, 2.83321334]), rtol=.01)), \"y_trn has incorrect values! Make sure the current column name was given to act as the labels!\")\n",
    "    ])\n",
    "    \n",
    "TEST_data_prep()\n",
    "garbage_collect(['TEST_data_prep'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c50e130",
   "metadata": {
    "id": "0c50e130"
   },
   "source": [
    "# Metrics and Plots\n",
    "\n",
    "Like always, below are functions which will bes used to measure the model's performance, compute loss/cost values, and visualize any results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3765f9ca",
   "metadata": {
    "id": "3765f9ca"
   },
   "source": [
    "#### TODO 5\n",
    "\n",
    "Complete the following TODO by finishing the following performance metric functions: sum of squared error `sse()`, mean squared error `mse()`, and root mean square error `rmse()`.\n",
    "\n",
    "1. Implement the sum of squared errors (SSE) cost function by finishing the `sse()` function. Store output into `sse_`.\n",
    "\n",
    "$$\n",
    "SSE  = \\sum_{i=0}^m (\\hat{y} - y)^2\n",
    "$$ \n",
    "\n",
    "2. Implement the mean square error (MSE) loss function by finishing the `mse()` function. Store output into `mse_`.\n",
    "$$\n",
    "MSE  = \\frac{1}{m}\\sum_{i=0}^m (\\hat{y} - y)^2\n",
    "$$ \n",
    "\n",
    "3. Implement the root mean squared error (RMSE) loss function by finishing the `rmse()` function. Store output into `rmse_`.\n",
    "$$\n",
    "RMSE  = \\sqrt{\\frac{1}{m}\\sum_{i=0}^m (\\hat{y} - y)^2}\n",
    "$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d50ca79",
   "metadata": {
    "id": "1d50ca79"
   },
   "outputs": [],
   "source": [
    "def reshape_labels(y):\n",
    "    if len(y.shape) != 2:\n",
    "        y = y.reshape(-1, 1)\n",
    "    return y\n",
    "\n",
    "def sse(y: np.ndarray, y_hat: np.ndarray):\n",
    "    # Checks if y or y_hat need to be reshaped into 2D arrays\n",
    "    y = reshape_labels(y)\n",
    "    y_hat = reshape_labels(y_hat)\n",
    "    \n",
    "    # TODO 5.1\n",
    "    sse_ = np.sum((y_hat-y)**2)\n",
    "    return sse_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537c4a43",
   "metadata": {
    "id": "537c4a43"
   },
   "source": [
    "Run the below `TEST_sse` function to test your implementation of the `sse()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd2febd",
   "metadata": {
    "id": "efd2febd",
    "outputId": "5c776f72-eb28-4df6-d466-c4ecfe00eb04"
   },
   "outputs": [],
   "source": [
    "def TEST_sse():\n",
    "    rng = np.random.RandomState(0)\n",
    "    fake_y_hat = rng.rand(100, 1)\n",
    "    fake_y = rng.rand(100, 1)\n",
    "    \n",
    "    sse_ = sse(y=fake_y, y_hat=fake_y_hat)\n",
    "    print(f\"Fake SSE: {sse_}\")\n",
    "\n",
    "    todo_check([\n",
    "        (np.isclose(sse_, 17.328, rtol=.01), 'sse value is wrong!'),\n",
    "    ])\n",
    "    \n",
    "TEST_sse()\n",
    "garbage_collect(['TEST_sse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390b424e",
   "metadata": {
    "id": "390b424e"
   },
   "outputs": [],
   "source": [
    "def mse(y: np.ndarray, y_hat: np.ndarray):\n",
    "    y = reshape_labels(y)\n",
    "    y_hat = reshape_labels(y_hat)\n",
    "    \n",
    "    # TODO 5.2\n",
    "    mse_ = (np.sum((y_hat-y)**2))/len(y)\n",
    "    return mse_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2db7b7f",
   "metadata": {
    "id": "c2db7b7f"
   },
   "source": [
    "Run the below `TEST_mse` function to test your implementation of the `mse()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e909452c",
   "metadata": {
    "id": "e909452c",
    "outputId": "7247020a-6647-400f-84c2-3dc3e1c1b38d"
   },
   "outputs": [],
   "source": [
    "def TEST_mse():\n",
    "    rng = np.random.RandomState(0)\n",
    "    fake_y_hat = rng.rand(100, 1)\n",
    "    fake_y = rng.rand(100, 1)\n",
    "\n",
    "    mse_ = mse(y=fake_y, y_hat=fake_y_hat)\n",
    "    print(f\"Fake MSE: {mse_}\")\n",
    "\n",
    "    todo_check([\n",
    "        (np.isclose(mse_, 0.173, rtol=.01),'mse_ value is wrong!'),\n",
    "    ])\n",
    "    \n",
    "TEST_mse()\n",
    "garbage_collect(['TEST_mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b621d3",
   "metadata": {
    "id": "a0b621d3"
   },
   "outputs": [],
   "source": [
    "def rmse(y: np.ndarray, y_hat: np.ndarray):\n",
    "    y = reshape_labels(y)\n",
    "    y_hat = reshape_labels(y_hat)\n",
    "    \n",
    "    # TODO 5.3\n",
    "    rmse_ = ((np.sum((y_hat-y)**2))/len(y))**.5\n",
    "    return rmse_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cc036e",
   "metadata": {
    "id": "83cc036e"
   },
   "source": [
    "Run the below `TEST_rmse` function to test your implementation of the `rmse()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acde76fa",
   "metadata": {
    "id": "acde76fa",
    "outputId": "623e3753-5376-4a4c-82f6-8d6aff7c3f9a"
   },
   "outputs": [],
   "source": [
    "def TEST_rmse():\n",
    "    rng = np.random.RandomState(0)\n",
    "    fake_y_hat = rng.rand(100, 1)\n",
    "    fake_y = rng.rand(100, 1)\n",
    "\n",
    "    rmse_ = rmse(y=fake_y, y_hat=fake_y_hat)\n",
    "    print(f\"Fake RMSE: {rmse_}\")\n",
    "\n",
    "    todo_check([\n",
    "        (np.isclose(rmse_, 0.416, rtol=.01), 'rmse_ value is wrong!'),\n",
    "    ])\n",
    "    \n",
    "TEST_rmse()\n",
    "garbage_collect(['TEST_rmse'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79fa430",
   "metadata": {
    "id": "c79fa430"
   },
   "source": [
    "Below we define the `unlog()` function for you. The equation $e^{x} -1$ is used for reversing the log transform. Recall we use this to undo the log transformation done to our target 'area'. Further, we use this function in conjunction with RMSE to get the average loss in the original units hectare (ha)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4592a061",
   "metadata": {
    "id": "4592a061"
   },
   "outputs": [],
   "source": [
    "def unlog(x):\n",
    "    return np.exp(x) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c39ae6",
   "metadata": {
    "id": "e6c39ae6"
   },
   "source": [
    "Next, we quickly define the `performance_measures()` function which will automatically compute the SSE, MSE, and RMSE using the functions you implemented above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e4aaa9",
   "metadata": {
    "id": "d4e4aaa9"
   },
   "outputs": [],
   "source": [
    "def performance_measures(y: np.ndarray, y_hat: np.ndarray) -> Tuple[np.ndarray]:   \n",
    "    sse_ = sse(y=y, y_hat=y_hat)\n",
    "    mse_ = mse(y=y, y_hat=y_hat)\n",
    "    rmse_ = rmse(y=y, y_hat=y_hat)\n",
    "    \n",
    "    return sse_, mse_, rmse_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89e9b42",
   "metadata": {
    "id": "b89e9b42"
   },
   "source": [
    "Lastly, we quickly define the function `analyze()` function which computes all the loss/cost function scores and plots our targets and predictions. \n",
    "\n",
    "There are two plots that we will use. The first simply plots the targets and predictions on top of each other while the second plots the targets against the predictions where the targets are on the y-axis and predictions are on the x-axis.\n",
    "\n",
    "The second plot is a QQ plot (see below image). We use the QQ plot to plot the targets against the predictions. The diagonal line indicates the line of zero error. The closer each data point is to the line the lower the loss. Thus, the closer the prediction is to the actual target value. Good performance is indicated by the majority of the data points being close to the diagonal line.\n",
    "<img src=\"https://miro.medium.com/max/384/1*pmqp-wtSjUgPHCBs1Mm7Xg.jpeg\" width=300 height=300>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135d43dd",
   "metadata": {
    "id": "135d43dd"
   },
   "outputs": [],
   "source": [
    "def analyze(\n",
    "    y: np.ndarray,\n",
    "    y_hat: np.ndarray,  \n",
    "    title: str, \n",
    "    dataset: str,\n",
    "    xlabel: str = None,\n",
    "    ylabel: str = None\n",
    ") -> Tuple[np.ndarray, float, float, float]:\n",
    "    \"\"\" Plot your results and compute perforamnce measures \n",
    "        using your predictions and labels.\n",
    "        \n",
    "        Args:\n",
    "            y: Labels\n",
    "            \n",
    "            y_hat: Predictions\n",
    "            \n",
    "            title: Title of plot\n",
    "            \n",
    "            dataset: Name of dataset being plotted \n",
    "                (typically training, validaiton, or test).\n",
    "                \n",
    "            xlabel: X-axis label for the 1st plot\n",
    "            \n",
    "            ylabel - Y-axis label for both plots\n",
    "    \"\"\"\n",
    "    y = reshape_labels(y)\n",
    "    y_hat = reshape_labels(y_hat)\n",
    "    \n",
    "    sse_, mse_, rmse_ = performance_measures(y=y, y_hat=y_hat)\n",
    "    \n",
    "    # plotting\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(15,7))\n",
    "    fig.suptitle(title, fontsize=15)\n",
    "    axs[0].plot(y, 'ob', label='Target')\n",
    "    axs[0].plot(y_hat, 'xr', label='Prediction')\n",
    "    axs[0].set_xlabel(xlabel)\n",
    "    axs[0].set_ylabel(ylabel)\n",
    "    axs[0].legend()\n",
    "\n",
    "    y1 = min(y)\n",
    "    y2 = max(y)\n",
    "    line = np.arange(y1,y2)\n",
    "    axs[1].scatter(y, y_hat, label=\"True vs Predicted\", color='b', marker='*')\n",
    "    axs[1].set_xlabel(\"Targets\")\n",
    "    axs[1].set_ylabel(ylabel)\n",
    "    # axs[1].set_ylim(y1, y2)\n",
    "    # axs[1].set_xlim(y1, y2)\n",
    "    axs[1].plot(line, line, label = \"Line of Zero Error\", color='r')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"{dataset} MSE: {mse_}\")\n",
    "    print(f\"{dataset} RMSE: {rmse_}\")\n",
    "    print(f\"{dataset} SSE: {sse_}\\n\")\n",
    "    \n",
    "    print(f\"{dataset} unlogged MSE: {unlog(mse_)} ha\")\n",
    "    print(f\"{dataset} unlogged RMSE: {unlog(rmse_)} ha\")\n",
    "    \n",
    "    return sse_, mse_, rmse_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1106c911",
   "metadata": {
    "id": "1106c911"
   },
   "source": [
    "# Neural Networks\n",
    "Let's get onto implementing the neural network algorithm. A short summary of neural networks will be given below. For more details see the notes.\n",
    "\n",
    "A general neural network framework can be seen in the below image. The below neural network is an example of a 2 layer neural network. We'll be working with 2 layer neural networks throughout this lab. \n",
    "\n",
    "\n",
    "<img src=\"https://images.squarespace-cdn.com/content/v1/5ccb715016b640627a1c2782/1586907440135-6INS2V3VS31ICY3TNUIW/ai-artificial-neural-network-alex-castrounis.png?format=500w\" height=400 width=400>\n",
    "\n",
    "**Neurons**\n",
    "\n",
    "Recall that a neural network is made of neurons. The structure of a neuron is given by the below image. \n",
    "\n",
    "<img src=\"https://live.staticflickr.com/65535/51633984258_9b95f324ce.jpg\" width=\"300\" height=\"150\" alt=\"artifcal-neuron\">\n",
    "\n",
    "The 4 basics steps all neurons utilize are given as follows:\n",
    "1. First, each neuron takes input from every neuron in the previous layer represented by $x$ or $a$. The 1st hidden layer always takes in the data $x$ as input.\n",
    "2. Next, each input is assigned a weight just like we have seen in previous algorithms. For clarity sack, we will keep the bias separate from the weights even though they can be combined as we have done in the past. Using the weights, a linear combination can be taken between the weights and inputs where the bias is also added such that we get the following: $z = \\wv^\\top \\xv + b$. Recall $z$ represents the continuous output of a neuron (i.e., the output after the linear combination of inputs). This linear combination of inputs is nothing new, we have seen this time and time again with linear regression and classification algorithms alike.\n",
    "3. Next, an activation function $g(\\cdot)$is applied to $z$ producing the  neuron output $a$ where $a = g(z)$. The activation function is often picked to be a non-linear function. We do this in order to convert our equation from a linear equation to a non-linear equation (more on this later). \n",
    "4. Finally, after applying the activation function, the output $a$ is passed to the next layer and acts as the inputs for the next layer. \n",
    "\n",
    "**Layers**\n",
    "\n",
    "Lastly, recall that a neuron network can stack multiple neurons to create layers. For a basic 2 layer neural network there are two main layers: the hidden layer and the output layer.\n",
    "\n",
    "**Input layer:**\n",
    "\n",
    "The layer which takes in all the inputs of the data is referred to as the *input layer*. While this layer is not counted as an actual layer as it contains no neurons. It is often referred to as the input layer regardless. This layer is used to represent all the data features that will be input into the actual neural network. In other words, we can think of this layer as representing the input data as a matrix $\\Xm$ (multiple data samples) or a vector $\\xv$ (a single data sample) where each blue circle in the input layer image above can be thought of as a single feature in the data. Thus, the above data would have 3 features.\n",
    "\n",
    "**Hidden layer:**\n",
    "\n",
    "The first actual layer is called the *hidden layer*. The hidden layer acts as a intermediate layer between the input layer and output layers. A hidden layer is the 1st real layer in a neural network. The hidden layer contains neurons which are more specifically referred to as *hidden neurons*, also known as *hidden units*. This is partly done to differentiate the hidden neurons from the output neurons. \n",
    "\n",
    "**Output layer:**\n",
    "\n",
    "The second and last layer is called the *output layer*. The output layer contains neurons which produce the outputs of the neural network. In other words, the output layer is responsible for computing the predictions $\\hat{y}$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5944fa",
   "metadata": {
    "id": "ba5944fa"
   },
   "source": [
    "## Activation Functions\n",
    "\n",
    "Before we can implement neural networks we need to define our activation functions we'll be using in this lab. Recall each neuron can have an activation function. **The KEY element of neurons in neural networks are non-linear activation functions.** By introducing non-linearity with non-linear activation functions, neural networks are able to learn more complex non-linear models. \n",
    "\n",
    "\n",
    "Below are classes who have two static methods `activation()` and `derivative()`. Here the  `activation()`  static method computes the equation of the activation function while the `derivative()` static method computes the derivative of the activation function. We'll use the `activation()` static method for the feed-forward processes and the `derivative()` static method for the feedback process.\n",
    "\n",
    "*If you don't know what a static method is, see this [post](https://pythonbasics.org/static-method/).*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d144b3e1",
   "metadata": {
    "id": "d144b3e1"
   },
   "source": [
    "### Identity/Linear\n",
    "Let's start by implementing the simplest activation function which is a linear activation function referred to as the identity/linear activation function. Recall, this activation function is more of a placeholder activation function when you want to apply no activation function.\n",
    "\n",
    "Since we are performing regression we'll use this activation in the output layer as we DO NOT want to apply any activation function to our predictions. This because regression problems typically require that we predict continuous values (values between $-\\infty$ and $\\infty$) not values squashed within a certain value range!\n",
    "#### TODO 6\n",
    "Complete the TODO by implementing the `activation()` and `derivative()` static methods for the `Linear` class.\n",
    "\n",
    "1. Implement the `activation()` method to simply return the input `z`.\n",
    "\n",
    "\n",
    "2. Implement the `derivative()` method which returns the derivative of the linear activation function. Recall this is simply just an array of ones that is the same shape as the input `z`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad7966f",
   "metadata": {
    "id": "4ad7966f"
   },
   "outputs": [],
   "source": [
    "class Linear():\n",
    "    @staticmethod\n",
    "    def activation(z):\n",
    "        # TODO 6.1\n",
    "        return z\n",
    "    \n",
    "    @staticmethod\n",
    "    def derivative(z):\n",
    "        # TODO 6.2\n",
    "        return np.ones(np.shape(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8657d8b4",
   "metadata": {
    "id": "8657d8b4",
    "outputId": "23d4ebc0-7dfe-4c4b-e050-0b891616df08"
   },
   "outputs": [],
   "source": [
    "def TEST_Linear():\n",
    "    x = np.arange(-10, 10 , .2)\n",
    "    y = Linear.activation(x)\n",
    "    dy = Linear.derivative(x)\n",
    "\n",
    "    plt.plot(x ,dy, label='Derivative')\n",
    "    plt.plot(x ,y, label='Activation')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    todo_check([\n",
    "        (np.all(np.isclose(y[50:53], np.array([0. ,  0.2,  0.4]))), \"y values are incorrect\"),\n",
    "        (np.all(np.isclose(dy[50:53], np.array([1., 1., 1.]))), \"yd derivative values are incorrect\")\n",
    "    ])\n",
    "    \n",
    "TEST_Linear()\n",
    "garbage_collect(['TEST_Linear'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb79008e",
   "metadata": {
    "id": "eb79008e"
   },
   "source": [
    "### Sigmoid\n",
    "\n",
    "Next, we'll need to implement the  non-linear sigmoid activation function which squashes values between 0 and 1. Recall, the sigmoid activation function equation is given as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "g(z) &= \\frac{1}{1 + e^{-z}} \\\\\n",
    "&= \\frac{e^z}{1 + e^{z}}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Further the derivative of the sigmoid activation is given as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "g'(z) &= \\frac{1}{1 + e^{-z}} (1 - \\frac{1}{1 + e^{-z}}) \\\\\n",
    "&= g(z)(1- g(z))\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "#### TODO 7\n",
    "Complete the TODO by implementing the `activation()` and `derivative()` static methods for the `Sigmoid` class.\n",
    "\n",
    "1. Implement the `activation()` method by implementing the below version of the sigmoid function. Return the output of the equation.\n",
    "\n",
    "$$\n",
    "g(z) = \\frac{e^z}{1 + e^{z}}\n",
    "$$ \n",
    "\n",
    "2. Implement the `derivative()` method implementing the derivative of the sigmoid equation given above. Return the output of the equation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1296d8ad",
   "metadata": {
    "id": "1296d8ad"
   },
   "outputs": [],
   "source": [
    "class Sigmoid():\n",
    "    @staticmethod\n",
    "    def activation(z):\n",
    "        # TODO 7.1\n",
    "        e_z = np.exp(z - np.max(z))\n",
    "        return e_z / e_z.sum(axis=0)\n",
    "    \n",
    "    @staticmethod\n",
    "    def derivative(z):\n",
    "        # TODO 7.2\n",
    "        e = np.e**z\n",
    "        r = e/((1+e)**2)\n",
    "        return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3c2421",
   "metadata": {
    "id": "fd3c2421",
    "outputId": "6ad15eec-cd36-4199-affa-ec5b1c18fe36"
   },
   "outputs": [],
   "source": [
    "def TEST_Sigmoid():\n",
    "    x = np.arange(-10, 10 , .2)\n",
    "    y = Sigmoid.activation(x)\n",
    "    dy = Sigmoid.derivative(x)\n",
    "\n",
    "\n",
    "    plt.hlines(.5, xmin=-10, xmax=10, colors='black', linestyles='dotted')\n",
    "    plt.plot(x ,dy, label='Derivative')\n",
    "    plt.plot(x ,y, label='Activation')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    todo_check([\n",
    "        (np.all(np.isclose(y[50:53], np.array([0.5, 0.549834  , 0.59868766]))), \"y values are incorrect\"),\n",
    "        (np.all(np.isclose(dy[50:53], np.array([0.25, 0.24751657, 0.24026075]))), \"yd derivative values are incorrect\")\n",
    "    ])\n",
    "\n",
    "\n",
    "TEST_Sigmoid()\n",
    "garbage_collect(['TEST_Sigmoid'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf1c11e",
   "metadata": {
    "id": "caf1c11e"
   },
   "source": [
    "### Tanh\n",
    "\n",
    "Next, we'll need to implement the  non-linear $\\tanh$  activation function which squashes values between -1 and 1. Recall, the $\\tanh$  activation function equation is given as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "g(z) &= \\tanh(z) \\\\\n",
    "&= \\frac{e^z - e^{-z}}{e^z + e^{-z}}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Further, the derivative of the $\\tanh$  activation is given as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "g'(z) &=  1 - \\tanh^2(z)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "#### TODO 8\n",
    "\n",
    "Complete the TODO by implementing the `activation()` and `derivative()` static methods for the `Tanh` class.\n",
    "\n",
    "1. Implement the `activation()` method by implementing the $\\tanh$  equation given above. Return the output of the equation.\n",
    "    1. Hint: NumPy has already implemented a function for $\\tanh$\n",
    "\n",
    "\n",
    "2. Implement the `derivative()` method implementing the derivative of the $\\tanh$ equation given above. Return the output of the equation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a16353b",
   "metadata": {
    "id": "3a16353b"
   },
   "outputs": [],
   "source": [
    "class Tanh():\n",
    "    @staticmethod\n",
    "    def activation(z):\n",
    "        # TODO 8.1\n",
    "        e = np.e**z\n",
    "        en = np.e**(-1*z)\n",
    "        tanh = (e - en)/(e + en)\n",
    "        return tanh\n",
    "    \n",
    "    @staticmethod\n",
    "    def derivative(z):\n",
    "        # TODO 8.2\n",
    "        e = np.e**z\n",
    "        en = np.e**(-1*z)\n",
    "        tanh = (e - en)/(e + en)\n",
    "        ret = 1 - (tanh)**2\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d80479",
   "metadata": {
    "id": "a1d80479",
    "outputId": "bc0c0612-9304-473c-c65b-7383117d7f84"
   },
   "outputs": [],
   "source": [
    "def TEST_Tanh():\n",
    "    x = np.arange(-10, 10 , .2)\n",
    "    y = Tanh.activation(x)\n",
    "    dy = Tanh.derivative(x)\n",
    "\n",
    "    plt.hlines(0, xmin=-10, xmax=10, colors='black', linestyles='dotted')\n",
    "    plt.plot(x ,dy, label='Derivative')\n",
    "    plt.plot(x ,y, label='Activation')\n",
    "    plt.legend()\n",
    "    plt.show();\n",
    "\n",
    "\n",
    "    todo_check([\n",
    "        (np.all(np.isclose(y[50:53], np.array([0.,  0.19737532,  0.37994896]))), \"y values are incorrect\"),\n",
    "        (np.all(np.isclose(dy[50:53], np.array([1., 0.96104298, 0.85563879]))), \"yd derivative values are incorrect\")\n",
    "    ])\n",
    "    \n",
    "TEST_Tanh()\n",
    "garbage_collect(['TEST_Tanh'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092dc3d4",
   "metadata": {
    "id": "092dc3d4"
   },
   "source": [
    "## Forward pass\n",
    "\n",
    "Now it's time to implement the feed-forward processes for our neural network algorithm. To do so we are going to look at how data flows through an 2 layer neural network. We'll use the below image of a 2 layer neural network as a schematic for our initial implementation. However, as we'll see, adding additional neurons will be trivial thanks to vector notation.\n",
    "\n",
    "**This section will be notation heavy. If you get lost please refer to the notes or the table of notation located at the top of this notebook.**\n",
    "\n",
    "Below is image of the base single layer neural network we will use to practice implementation. Note that it corresponds to our Forest Fire dataset as it has 29 feature inputs!\n",
    "\n",
    "<img src=\"https://live.staticflickr.com/65535/51990549288_5a1aa0b89e_k.jpg\" width=\"500\" height=\"409\" alt=\"single_layer_nn\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6533b54d",
   "metadata": {
    "id": "6533b54d"
   },
   "source": [
    "### Hidden layer\n",
    "\n",
    "Alright, the first step in computing the predictions for a neural network is compute the output of the first layer. In our case, this is the first and only hidden layer. Below is the image of the hidden layer and the input layer. The input layer will provide the inputs that the hidden layer will use to compute its outputs. \n",
    "\n",
    "The hidden layer will have 2 hidden units or neurons where each hidden neuron will use the sigmoid activation function.\n",
    "\n",
    "<img src=\"https://live.staticflickr.com/65535/51990489126_be1cd013c6_h.jpg\" width=\"300\" height=\"209\" alt=\"signle_nn_hidden_layers\">\n",
    "\n",
    "*Note, the red arrow in the picture points in the direction information is flowing. In this case, information is flowing forwards.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97162ae0",
   "metadata": {
    "id": "97162ae0"
   },
   "source": [
    "#### TODO 9\n",
    "\n",
    "Complete the TODO by implementing each equation to compute the output of the hidden layer using the training data `X_trn` and `y_trn`.\n",
    "\n",
    "**Initialization**\n",
    "1. Initialize the weights $\\Wm^{[1]}$ for the hidden layer. Store the output into `W1`. To do so, use the following tips:\n",
    "    1. The inputs for the hidden layer correspond to the **number of features** in the passed input data, in this case `X_trn`. \n",
    "    1. The hidden layer should have 2 hidden units or neurons.\n",
    "\n",
    "    1. The weights should have the shape (neurons, inputs). Think about what the inputs are and how many neurons the hidden layer has.\n",
    "    1. Use the NumPy's `rng.uniform()` function ([docs](https://numpy.org/doc/stable/reference/random/generated/numpy.random.uniform.html)) to randomly generate the weight matrix of shape (neurons, inputs). The weight values should be restricted between the range of -0.5 (minimum) and 0.5 (maximum). \n",
    "2. Initialize the biases $\\bv^{[1]}$ for the hidden layer. Store the output into `b1`. To do so, use the following tips:\n",
    "    1. The biases should be a vector of all 1s with the shape (neurons, 1). Try using `np.ones()` ([docs](https://numpy.org/doc/stable/reference/generated/numpy.ones.html)).\n",
    "\n",
    "**Computing linear combinations**\n",
    "\n",
    "3. Compute $\\Zm^{[1]}$ the linear combination of the input features with the weights for ALL neurons in the hidden layer simultaneously using `X_trn` as the input. Convert the below equation to do so where you might need to transpose certain variables. Store the output into `Z1`.\n",
    "    1. Hint: The input data `X_trn` must be of shape (features, data samples). However, to work with our neural network algorithm, we need `X_trn` to have the shape (data samples, features).\n",
    "$$\n",
    "\\Zm^{[1]} = \\Wm^{[1]} \\Xm+ \\bv^{[1]}.\n",
    "$$\n",
    "\n",
    "**Computing layer outputs**\n",
    "\n",
    "4. Compute $\\Am^{[1]} $ the output of ALL neurons in the hidden layer simultaneously by applying the sigmoid activation function to the linear combination output $\\Zm^{[1]}$. Convert the below equation where $g$ is the `Sigmoid` class and the `activation()` static method applies the activation function equation. Store the output into `A1`.\n",
    "    1. Hint: To call a static method such as the `activation()` method for the `Sigmoid` class you DO NOT need to instantiate the class. See an example in this [post](https://stackoverflow.com/questions/11759269/calling-static-method-in-python).\n",
    "\n",
    "$$\n",
    "\\Am^{[1]} = g(\\Zm^{[1]})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f847a9",
   "metadata": {
    "id": "28f847a9",
    "outputId": "0d95e34b-c92f-4150-c61c-659ac08ac885"
   },
   "outputs": [],
   "source": [
    "data = data_prep(df=forestfire_df, label_name='area', return_array=True)\n",
    "X_trn, y_trn, _, _, _,_ = data\n",
    "\n",
    "print(f\"X_trn shape: {X_trn.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844b5aec",
   "metadata": {
    "id": "844b5aec",
    "outputId": "a78fc2eb-6570-441c-a6bb-47282eb031d1"
   },
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(0)\n",
    "\n",
    "# TODO 9.1\n",
    "W1 = rng.uniform(-.5,.5, (2, len(X_trn[0])))\n",
    "\n",
    "# TODO 9.2\n",
    "b1 = np.ones((2,1))\n",
    "\n",
    "print(f\"Hidden layer weights shape: {W1.shape}\")\n",
    "print(f\"Hidden layer bias shape: {b1.shape}\")\n",
    "\n",
    "todo_check([\n",
    "    (W1.shape == (2, 29), \"W1 has the wrong shape\"),\n",
    "    (b1.shape == (2, 1), \"b1 has the wrong shape\"),\n",
    "    (np.all(np.isclose(W1[0][:3], np.array([0.0488135 , 0.21518937, 0.10276338]), rtol=.01)), \"W1 has incorrect values\"),\n",
    "    (np.all(b1 == 1), 'b1 has incorrect values')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d059c6",
   "metadata": {
    "id": "54d059c6",
    "outputId": "865806b1-d7c6-47e9-e476-a0e3a602e6b5"
   },
   "outputs": [],
   "source": [
    "# TODO 9.3\n",
    "Z1 = W1@X_trn.T+b1\n",
    "\n",
    "print(f\"{Z1.shape} =  {W1.shape} @ {X_trn.T.shape} + {b1.shape}\")\n",
    "print(f\"Z1 shape: {Z1.shape}\")\n",
    "\n",
    "todo_check([\n",
    "    (Z1.shape == (2, 327), \"Z1 has the wrong shape\"),\n",
    "    (np.all(np.isclose(Z1[0][:3], np.array([1.10623323, 0.96034753, 0.46206967]), rtol=.01)), \"Z1 has incorrect values\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1584bf9",
   "metadata": {
    "id": "a1584bf9",
    "outputId": "657daedc-15df-4050-c2ba-087f31206ebb"
   },
   "outputs": [],
   "source": [
    "# TODO 9.4\n",
    "A1 = Sigmoid.activation(Z1)\n",
    "\n",
    "print(f\"A1 shape: {A1.shape}\")\n",
    "\n",
    "todo_check([\n",
    "    (A1.shape == (2, 327), \"A1 has the wrong shape\"),\n",
    "    (np.all(np.isclose(A1[0][:3], np.array([0.7514262 , 0.72319138, 0.61350504]), rtol=.01)), \"A1 has incorrect values\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefdeba7",
   "metadata": {
    "id": "aefdeba7"
   },
   "source": [
    "### Output Layer\n",
    "\n",
    "The final step in computing the predictions for a neural network is to compute the output of the 2nd layer (i.e., output layer). Below is the image of the hidden layer and the output layer. The hidden layer will provide the inputs that the output layer will use to compute its SINGLE output for each data sample.\n",
    "\n",
    "The output layer will have 1 hidden unit or neuron which utilizes the identity/linear activation function (i.e., no activation function).\n",
    "\n",
    "<img src=\"https://live.staticflickr.com/65535/51990775824_3574bc97a4_h.jpg\" width=\"300\" height=\"239\" alt=\"signle_nn_output_layer\">\n",
    "\n",
    "*Note, the red arrow in the picture points in the direction information is flowing. In this case, information is flowing forwards.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b51104",
   "metadata": {
    "id": "a0b51104"
   },
   "source": [
    "#### TODO 10\n",
    "\n",
    "Complete the TODO by implementing each equation to compute the output of the output layer using the output from the hidden layer `A1`.\n",
    "\n",
    "**Initialization**\n",
    "1. Initialize the weights $\\wv^{[2]}$ of the output layer. Store the output into `W2`. To do so, use the following tips:\n",
    "    1. The inputs for the output layer correspond to the number of outputs from the previous layer's neurons `A1`. \n",
    "    1. The output layer should have 1 neuron.\n",
    "    1. The weights should have the shape (neurons, inputs). Think about how many neurons were in the previous layer and how many neurons the output layer has.\n",
    "    1. Use the NumPy's `rng.uniform()` function ([docs](https://numpy.org/doc/stable/reference/random/generated/numpy.random.uniform.html)) to randomly generate the weight matrix of shape (neurons, inputs). The weight values should be restricted between the range of -0.5 (minimum) and 0.5 (maximum). \n",
    "2. Initialize the biases $\\bv^{[2]}$ of the output layer. Store the output into `b2`. To do so, use the following tips:\n",
    "    1. The biases should be a vector of all 1s with the shape (neurons, 1). Try using `np.ones()` ([docs](https://numpy.org/doc/stable/reference/generated/numpy.ones.html)).\n",
    "\n",
    "**Computing linear combinations**\n",
    "\n",
    "3. Compute $\\zv^{[2]} $ the linear combination of the input features with the weights for ALL neurons in the output layer simultaneously. Convert the below equation to do so. Store the output into `Z2`.\n",
    "\n",
    "$$\n",
    "\\zv^{[2]} = \\wv^{[2]} \\Am^{[1]} + \\bv^{[2]}.\n",
    "$$\n",
    "\n",
    "**Computing layer outputs**\n",
    "\n",
    "4. Compute $\\av^{[2]}$ the output of ALL neurons in the output layer simultaneously by applying the identity/linear activation function (i.e., no activation function) to the linear combination output $\\zv^{[1]}$. Convert the below equation to do so where $g$ is the `Linear` class and the `activation()` static method applies the activation function equation. Store the output into `A2`.\n",
    "    1. Hint: To call a static method such as the `activation()` method for the `Linear` class you DO NOT need to instantiate the class. See an example in this [post](https://stackoverflow.com/questions/11759269/calling-static-method-in-python).\n",
    "\n",
    "$$\n",
    "\\av^{[2]} = g(\\zv^{[2]})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e322ee8e",
   "metadata": {
    "id": "e322ee8e",
    "outputId": "b9218f3d-8288-45d3-8e6c-1d00a2018a5d"
   },
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(1)\n",
    "\n",
    "# TODO 10.1\n",
    "W2 = rng.uniform(-.5,.5,(1,2))\n",
    "# TODO 10.2\n",
    "b2 = np.ones((1,1))\n",
    "\n",
    "print(f\"Input activations A1 shape: {A1.shape}\")\n",
    "print(f\"Output layer weights shape: {W2.shape}\")\n",
    "print(f\"Output layer bias shape: {b2.shape}\")\n",
    "\n",
    "todo_check([\n",
    "    (W2.shape == (1, 2), \"W2 has the wrong shape\"),\n",
    "    (b2.shape == (1, 1), \"b1 has the wrong shape\"),\n",
    "    (np.all(np.isclose(W2[0][:3], np.array([-0.082978  ,  0.22032449]), rtol=.01)), \"W2 has incorrect values\"),\n",
    "    (np.all(b2 == 1), \"b1 has incorrect values\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b01fc05",
   "metadata": {
    "id": "8b01fc05",
    "outputId": "515713fe-d149-4ae5-d058-a7f28871de30"
   },
   "outputs": [],
   "source": [
    "# TODO 10.3\n",
    "Z2 = W2@A1+b2\n",
    "\n",
    "print(f\"{Z2.shape} =  {W2.shape} @ {A1.shape} + {b2.shape}\")\n",
    "print(f\"Z2 shape: {Z2.shape}\")\n",
    "\n",
    "todo_check([\n",
    "    (Z2.shape == (1, 327), \"Z2 has the wrong shape\"),\n",
    "    (np.all(np.isclose(Z2[0][:3], np.array([1.10726381, 1.06542374, 1.13297247]), rtol=.01)), \"Z2 has incorrect values\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bffb65",
   "metadata": {
    "id": "12bffb65",
    "outputId": "5b411779-070a-47ae-d56a-4ca46f84d7b2"
   },
   "outputs": [],
   "source": [
    "# TODO 10.4\n",
    "A2 = Linear.activation(Z2)\n",
    "\n",
    "print(f\"A2 shape: {A2.shape}\")\n",
    "\n",
    "todo_check([\n",
    "    (A2.shape == (1, 327), \"A2 has the wrong shape\"),\n",
    "    (np.all(np.isclose(A2[0][:3], np.array([1.10726381, 1.06542374, 1.13297247]), rtol=.01)), \"A2 has incorrect values\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62aa6516",
   "metadata": {
    "id": "62aa6516"
   },
   "source": [
    "Notice we need to transpose `A2` the output from the output layer to make the predictions match the shape of the original labels `y_trn` (data samples, 1). We have to do this because we transposed the inputs to be (data samples, features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1dbe0ea",
   "metadata": {
    "id": "c1dbe0ea",
    "outputId": "602c81d6-9b7d-42eb-c931-5ccf0492dbcc"
   },
   "outputs": [],
   "source": [
    "A2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437e01db",
   "metadata": {
    "id": "437e01db",
    "outputId": "6da3daf9-86f9-4d6b-9ddd-31ad355113a7"
   },
   "outputs": [],
   "source": [
    "y_trn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530855e0",
   "metadata": {
    "id": "530855e0",
    "outputId": "0b629e0f-7758-431d-dc7f-a896ac922a78"
   },
   "outputs": [],
   "source": [
    "y_hat = A2.T\n",
    "y_hat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae0c37b",
   "metadata": {
    "id": "6ae0c37b"
   },
   "outputs": [],
   "source": [
    "garbage_collect(['data', 'X_trn', 'y_trn', 'y_hat', 'A2', 'Z2', 'b2', 'W2', 'rng', 'A1', 'Z1', 'b1', 'W1', 'n_input_features', 'hidden_neurons'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0913be5d",
   "metadata": {
    "id": "0913be5d"
   },
   "source": [
    "### Putting it all together\n",
    "\n",
    "We can put all the code together by putting the weight and bias initializations into a function called `init_weights()` and the forward pass code into a function called `forward()`.\n",
    "\n",
    "The `init_weights()` function will simply initialize the weights and biases for each layer for us and return them in a `weights` dictionary and a `bias` dictionary. The `forward()` function will apply the forward pass for all layers starting from the input layer and ending with the output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54e783b",
   "metadata": {
    "id": "c54e783b"
   },
   "source": [
    "#### TODO 11\n",
    "Complete the TODO by moving the code you used for prior TODOs into the `init_weights()` function.\n",
    "\n",
    "1. Initialize the weights using the instructions given in `TODO 10.1`. Use the `n_inputs_features`, `hidden_neurons`, and `output_neurons` arguments to determine the weight shapes for the hidden and output layers. Store the hidden layer weights into the dictionary `W` with the key `W1`.  Store the output layer weights into the dictionary `W` with the key `W2`.\n",
    "\n",
    "\n",
    "2. Initialize the biases  using the instructions given in `TODO 10.2`. Use the  `hidden_neurons` and `output_neurons` arguments to determine bias shapes for the hidden and output layers. Store the hidden layer biases into the dictionary `b` with the key `b1`.  Store the output layer biases into the dictionary `b` with the key `b2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71976f2c",
   "metadata": {
    "id": "71976f2c"
   },
   "outputs": [],
   "source": [
    "def init_weights(\n",
    "    n_input_features: int, \n",
    "    hidden_neurons: int, \n",
    "    output_neurons: int, \n",
    "    seed=0) -> Tuple[dict, dict]:\n",
    "    \"\"\" Initilize a two layer neural network's weights and baises\n",
    "        \n",
    "        Args:\n",
    "            n_input_features: The number of input features\n",
    "\n",
    "            hidden_neurons: The number of hidden neurons or units to \n",
    "                use in the hidden layer.\n",
    "\n",
    "            output_neurons: The number of output neurons to use in \n",
    "                the output layer.\n",
    "    \"\"\"\n",
    "    \n",
    "    W = {}\n",
    "    b = {}\n",
    "    \n",
    "    rng = np.random.RandomState(seed)\n",
    "    # TODO 11.1\n",
    "    W['W1'] = rng.uniform(-.5,.5,(hidden_neurons, n_input_features))\n",
    "    W['W2'] = rng.uniform(-.5,.5,(output_neurons, hidden_neurons))\n",
    "    # TODO 11.2\n",
    "    b['b1'] = np.ones((hidden_neurons, 1))\n",
    "    b['b2'] =np.ones((output_neurons, 1))\n",
    "    \n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784075e1",
   "metadata": {
    "id": "784075e1"
   },
   "source": [
    "Run the below `TEST_init_weights()` function to test your implementation of the `init_weights()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793f71df",
   "metadata": {
    "id": "793f71df",
    "outputId": "75d88b6f-d3c2-44be-9530-f55826caa7a9"
   },
   "outputs": [],
   "source": [
    "def TEST_init_weights():\n",
    "    # Get training data\n",
    "    data = data_prep(df=forestfire_df,label_name='area', return_array=True)\n",
    "    X_trn, y_trn, _, _, _,_ = data\n",
    "\n",
    "    # Init weights and baises\n",
    "    W, b = init_weights(\n",
    "        n_input_features=X_trn.shape[1],\n",
    "        hidden_neurons=2,\n",
    "        output_neurons=1\n",
    "    )\n",
    "    \n",
    "    display(W)\n",
    "    display(b)\n",
    "\n",
    "    todo_check([\n",
    "        ('W1' in W, \"W dict is missing key 'W1'\"),\n",
    "        ('W2' in W, \"W dict is missing key 'W2'\"),\n",
    "        ('b1' in b, \"b dict is missing key 'b1'\"),\n",
    "        ('b2' in b, \"b dict is missing key 'b2'\"),\n",
    "        (W['W1'].shape == (2, X_trn.shape[1]), \"W['W1'] has the wrong shape\"),\n",
    "        (W['W2'].shape == (1, 2), \"W['W2'] has the wrong shape\"),\n",
    "        (b['b1'].shape == (2, 1), \"b['b1'] has the wrong shape\"),\n",
    "        (b['b2'].shape == (1, 1), \"b['b2'] has the wrong shape\"),\n",
    "        (np.all(np.isclose(W['W1'][0][:3], np.array([0.0488135 , 0.21518937, 0.10276338]), rtol=.01)), \"W['W1'] has incorrect values\"),\n",
    "        (np.all(b['b1']  == 1), \"b['b1'] has incorrect values\"),\n",
    "        (np.all(np.isclose(W['W2'][0][:3], np.array([-0.03368923, -0.25557441]), rtol=.01)), \"W['W2'] has incorrect values\"),\n",
    "        (np.all(b['b2'] == 1), \"b['b2'] has incorrect values\"),\n",
    "    ])\n",
    "    \n",
    "TEST_init_weights()\n",
    "garbage_collect(['TEST_init_weights'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98390858",
   "metadata": {
    "id": "98390858"
   },
   "source": [
    "#### TODO 12\n",
    "Complete the TODO by recoding the code you used for prior TODOs inside the `forward()` function. Notice, the weights and biases are stored in dictionaries now. Further, the activation function for ALL the hidden layer neurons is passed to the `g_hidden` argument while the activation function for ALL the output layer neurons is passed to the `g_output` argument. You'll need to update the code to match these changes.\n",
    "\n",
    "**Input layer**\n",
    "\n",
    "1. Store the transposed input features `X` into the dictionary `As` with the key `A0`. Recall, neural networks only take input features with the shape (features, data samples). Further, the notation $\\Am^{[0]}$ is often used to represent $\\Xm$ as the output for the input layer.\n",
    "\n",
    "**Hidden layer**\n",
    "\n",
    "Inputs are stored in `As['A0']`, weights are stored in `W['W1']` and biases are stored in `b['b1']` and activation function for the neurons is stored in `g_hidden` argument.\n",
    "\n",
    "2. Compute $\\Zm^{[1]}$ the linear computation of inputs for ALL neurons using `As['A0']`. Store the output into the dictionary `Zs` with the key `Z1`.\n",
    "$$\n",
    "\\Zm^{[1]} = \\Wm^{[1]} \\Am^{[0]}+ \\bv^{[1]}.\n",
    "$$\n",
    "3. Compute $\\Am^{[1]}$ the activation output for ALL neurons. Store the output into the dictionary `As` with the key `A1`.\n",
    "$$\n",
    "\\Am^{[1]} = g(\\Zm^{[1]})\n",
    "$$\n",
    "\n",
    "**Output layer**\n",
    "\n",
    "Inputs are stored in `As['A1']`, weights are stored in `W['W2']`, biases are stored in `b['b2']` and activation function for the neuron is stored in `g_output` argument.\n",
    "\n",
    "4. Compute $\\zv^{[2]}$ the linear computation of inputs for ALL neurons. Store the output into the dictionary `Zs` with the key `Z2`.\n",
    "$$\n",
    "\\zv^{[2]} = \\wv^{[2]} \\Am^{[1]} + \\bv^{[2]}.\n",
    "$$\n",
    "5. Compute $\\av^{[2]}$ the activation output for ALL neurons. Store the output into the dictionary `As` with the key `A2`.\n",
    "$$\n",
    "\\av^{[2]} = g(\\zv^{[2]})\n",
    "$$\n",
    "6. Transpose the output layer output `As['A2']` to make predictions match the shape of the ground truth which is (data samples, 1). Store the output into `y_hat`.\n",
    "\n",
    "$$\n",
    "\\hat{\\yv} = \\av^{[2]\\top}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000ec5c1",
   "metadata": {
    "id": "000ec5c1"
   },
   "outputs": [],
   "source": [
    "def forward(\n",
    "    X: np.ndarray, \n",
    "    W: Dict[str, np.ndarray], \n",
    "    b:  Dict[str, np.ndarray], \n",
    "    g_hidden: object, \n",
    "    g_output: object) -> Tuple[np.ndarray, dict, dict]:\n",
    "    \"\"\" Forward pass for 2 layer neural network\n",
    "    \n",
    "        Args:\n",
    "            X: Input features\n",
    "            \n",
    "            W: Weights for all layers given as a dictionary\n",
    "            \n",
    "            b: Biases for all layers given as a dictionary\n",
    "            \n",
    "            g_hidden: Activation class containing a static method \n",
    "                called activation() for the first hidden layer.\n",
    "                \n",
    "            g_output: Activation class containing a static method \n",
    "                called activation() for the output layer.\n",
    "    \"\"\"\n",
    "    \n",
    "    Zs = {}\n",
    "    As = {}\n",
    "    # Input layer\n",
    "    # TODO 12.1\n",
    "    As['A0'] = X.T\n",
    "    \n",
    "    # hidden layer\n",
    "    # TODO 12.2\n",
    "    Zs['Z1'] = W['W1']@As['A0']+b[\"b1\"]\n",
    "    # TODO 12.3\n",
    "    As['A1'] = Sigmoid.activation(Zs['Z1'])\n",
    "    \n",
    "    # Output layer\n",
    "    # TODO 12.4\n",
    "    Zs['Z2'] = W['W2']@As['A1']+b[\"b2\"]\n",
    "    # TODO 12.5\n",
    "    As['A2'] = Linear.activation(Zs['Z2'])\n",
    "    # TODO 12.6\n",
    "    y_hat = As['A2'].T\n",
    "    \n",
    "    return y_hat, Zs, As"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288e17ea",
   "metadata": {
    "id": "288e17ea"
   },
   "source": [
    "Run the below code to test your implementation of the `forward()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225ce544",
   "metadata": {
    "id": "225ce544",
    "outputId": "c8dafb77-7c35-4a29-9316-21ce08c130ea"
   },
   "outputs": [],
   "source": [
    "# Get training data\n",
    "data = data_prep(df=forestfire_df, label_name='area', return_array=True)\n",
    "X_trn, y_trn, _, _, _,_ = data\n",
    "\n",
    "# Init weights and baises\n",
    "W, b = init_weights(\n",
    "    n_input_features=X_trn.shape[1],\n",
    "    hidden_neurons=2,\n",
    "    output_neurons=1\n",
    ")\n",
    "\n",
    "# Perform 1 forward pass\n",
    "y_hat, Zs, As = forward(\n",
    "    X=X_trn, \n",
    "    W=W,\n",
    "    b=b, \n",
    "    g_hidden=Sigmoid, \n",
    "    g_output=Linear\n",
    ")\n",
    "\n",
    "# Compute performance measures\n",
    "sse_score = sse(y=y_trn, y_hat=y_hat)\n",
    "mse_score = mse(y=y_trn, y_hat=y_hat)\n",
    "\n",
    "todo_check([\n",
    "    (y_hat.shape == (327, 1), \"y_hat has the wrong shape\"),\n",
    "    (As['A0'].shape == (29, 327), \"As['A0'] has the wrong shape\"),\n",
    "    (Zs['Z1'].shape == (2, 327), \"Zs['Z1'] has the wrong shape\"),\n",
    "    (Zs['Z2'].shape == (1, 327), \"Zs['Z2'] has the wrong shape\"),\n",
    "    (np.isclose(sse_score, 680.360, rtol=.01), 'sse_score is incorrect'),\n",
    "    (np.isclose(mse_score, 2.080612, rtol=.01),'mse_score is incorrect'),\n",
    "    (np.all(np.isclose(As['A1'][0][:3], np.array([0.7514262 , 0.72319138, 0.61350504]),rtol=.01)), \"As['A1'] has incorrect values\"),\n",
    "    (np.all(np.isclose(As['A2'][0][:3], np.array([0.77793242, 0.83013544, 0.76603249]),rtol=.01)), \"As['A2'] has incorrect values\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096abb39",
   "metadata": {
    "id": "096abb39"
   },
   "source": [
    "Below, you can see the output for the the `sse_score`, `mse_score`, `y_hat`, `W`, `b`, `Zs`, and `As`. Keep in mind, `W`, `b`, `Zs`, and `As` are all dictionaries where the keys correspond to layers!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a29049",
   "metadata": {
    "id": "47a29049",
    "outputId": "2171a314-9c2e-4696-a63f-52629fb2459f"
   },
   "outputs": [],
   "source": [
    "sse_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd91424f",
   "metadata": {
    "id": "bd91424f",
    "outputId": "c14efdfd-456e-4de6-8ced-79c0c77c4218"
   },
   "outputs": [],
   "source": [
    "mse_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1268ff",
   "metadata": {
    "id": "bd1268ff",
    "outputId": "600a5130-d86d-4e82-eb98-e80e41be2eb5"
   },
   "outputs": [],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1560ded3",
   "metadata": {
    "id": "1560ded3",
    "outputId": "4416ca4e-150f-47f7-a5e3-3e77764e7a8c"
   },
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0460cace",
   "metadata": {
    "id": "0460cace",
    "outputId": "df30bdeb-38c4-4b44-b22b-78c8b0206a89"
   },
   "outputs": [],
   "source": [
    "Zs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e35bdf0",
   "metadata": {
    "id": "4e35bdf0"
   },
   "source": [
    "Notice that the key `A0` corresponds to then input features transposed (features, data samples) and `A2` corresponds to the predictions transposed (1, data samples). Recall, `A1` is just the output of the hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81c7f3b",
   "metadata": {
    "id": "d81c7f3b",
    "outputId": "b73b4ce8-78cd-4d2b-b71a-248819ba3033"
   },
   "outputs": [],
   "source": [
    "As"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3648c7",
   "metadata": {
    "id": "ef3648c7"
   },
   "source": [
    "## Backward pass\n",
    "Now it's time to update the weights and biases using backpropagation. In order to do so remember we first need to perform the forward pass to get the predictions. We then need to assess the error/loss using our loss function, MSE in this case. \n",
    "\n",
    "Given the loss we can backpropagate the error through the network to update the weights in the direction that minimizes the loss (i.e., makes the total average loss smaller). Remember backpropagate here just means finding the gradient with respect to each of the parameters $\\Wm^{[1]}$, $\\bv^{[1]}$, $\\Wm^{[2]}$, and $\\bv^{[2]}$.\n",
    "\n",
    "Recall, we can write out all the equations for our 2 layer neural network including the loss as follows:\n",
    "\n",
    "\n",
    "1. Hidden layer (layer 1) equations\n",
    "\n",
    "$$\n",
    "\\Zm^{[1]} = \\Wm^{[1]} \\Xm + \\bv^{[1]}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Am^{[1]} = g(\\Zm^{[1]} )\n",
    "$$\n",
    "\n",
    "2. Output layer (layer 2) equations\n",
    "\n",
    "$$\n",
    "\\zv^{[2]} = \\wv^{[2]} \\Am^{[1]} + \\bv^{[2]}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\av^{[2]} = g(\\zv^{[2]})\n",
    "$$\n",
    "\n",
    "3. MSE loss\n",
    "$$\n",
    " \\hat{\\yv} = \\av^{[2]\\top} \n",
    "$$\n",
    "\n",
    "$$\n",
    "MSE = \\frac{1}{2m} (\\hat{\\yv} - {\\yv})^2 \n",
    "$$\n",
    "\n",
    "The goal will be to solve for $\\frac{\\partial MSE}{\\partial  \\Wm^{[1]}}$, $\\frac{\\partial MSE}{\\partial \\bv^{[1]}}$,  $\\frac{\\partial MSE}{\\partial \\wv^{[2]}}$, and $\\frac{\\partial MSE}{\\partial \\bv^{[2]}}$ where the MSE loss will be the \"parent\" or root equation where the computation of computing each gradient starts. Given the partial derivatives, we can then perform the following gradient descent updates.\n",
    "\n",
    "**Hidden layer updates**\n",
    "$$\n",
    "\\Wm^{[1]} = \\Wm^{[1]} - \\alpha \\nabla \\frac{\\partial MSE}{\\partial  \\Wm^{[1]}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\bv^{[1]} = \\bv^{[1]} - \\alpha \\nabla \\frac{\\partial MSE}{\\partial  \\bv^{[1]}}\n",
    "$$\n",
    "**Output layer updates**\n",
    "$$\n",
    "\\Wm^{[2]} = \\wv^{[2]} - \\alpha \\nabla \\frac{\\partial MSE}{\\partial  \\wv^{[2]}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\bv^{[2]} = \\bv^{[2]} - \\alpha \\nabla \\frac{\\partial MSE}{\\partial  \\bv^{[2]}}\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cc3929",
   "metadata": {
    "id": "b6cc3929"
   },
   "source": [
    "### MSE Derivative \n",
    "\n",
    "Before we start, we need to compute the derivative of the MSE loss function as it will act as the \"parent\" or root equation where each partial derivative equation will start from. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686d9ce1",
   "metadata": {
    "id": "686d9ce1"
   },
   "source": [
    "#### TODO 13\n",
    "Complete the TODO by implementing the derivative of the MSE loss function inside the `delta_mse()` function.\n",
    "\n",
    "1. Compute the derivative of the MSE loss function and return the output. The MSE derivative is given as follows:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial MSE}{\\partial \\hat{\\yv}} = \\hat{\\yv} - \\yv\n",
    "$$\n",
    "where $ \\hat{\\yv} = \\av^{[2]\\top}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6e578b",
   "metadata": {
    "id": "3b6e578b"
   },
   "outputs": [],
   "source": [
    "def delta_mse(y, y_hat):\n",
    "    # TODO 13.1\n",
    "    return y_hat - y\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f03ed7c",
   "metadata": {
    "id": "1f03ed7c"
   },
   "source": [
    "Run the below code to test your implementation of the `delta_mse()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f61d3af",
   "metadata": {
    "id": "1f61d3af",
    "outputId": "ed53a017-3c4b-4952-a510-1de68d20d0ed"
   },
   "outputs": [],
   "source": [
    "def TEST_delta_mse():\n",
    "    fake_y = np.array([[.2, 1.2, 3.4]])\n",
    "    fake_y_hat = np.array([[1.5, 1.1, 3.0]])\n",
    "    fake_delta_mse = delta_mse(fake_y, fake_y_hat)\n",
    "\n",
    "    todo_check([\n",
    "        (np.all(np.isclose(fake_delta_mse, np.array([[ 1.3, -0.1, -0.4]]), rtol=.01)), \"delta_mse has an incorrect value\")\n",
    "    ])\n",
    "    \n",
    "TEST_delta_mse()\n",
    "garbage_collect(['TEST_delta_mse'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9654b4",
   "metadata": {
    "id": "0d9654b4"
   },
   "source": [
    "### Output Layer\n",
    "\n",
    "To start off, we first compute the derivative of the parent function, the MSE loss, and then slowly compute the derivatives of each equation up until we reach the desired weight and bias parameter. The first parameters we will run into are the weights and biases for the output layer. Thus, we'll compute the partial derivatives $\\frac{\\partial MSE}{\\partial \\wv^{[2]}}$ and $\\frac{\\partial MSE}{\\partial \\bv^{[2]}}$ first as they are the first parameters we run into when propagating backwards through the network.\n",
    "\n",
    "This process is visually depicted with the below image where we are backpropagating through the network up to the end of the output layer.\n",
    "\n",
    "<img src=\"https://live.staticflickr.com/65535/51990775829_30775493eb_h.jpg\" width=\"300\" height=\"246\" alt=\"signle_nn_output_layers_back\">\n",
    "\n",
    "To compute the gradient or partial derivative of the weights $\\Wm^{[2]}$ and bias $\\bv^{[2]}$ we need the following equations:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "MSE &= \\frac{1}{2m} (\\av^{[2]\\top} - {\\yv})^2  \\\\ \n",
    " \\\\\n",
    "\\av^{[2]} &= g(\\zv^{[2]}) \\\\\n",
    "\\\\\n",
    "\\zv^{[2]} &= \\wv^{[2]} \\Am^{[1]} + \\bv^{[2]}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $\\hat{\\yv} = \\av^{[2]\\top}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686dd0c4",
   "metadata": {
    "id": "686dd0c4"
   },
   "source": [
    "<hr>\n",
    "\n",
    "**Weights**\n",
    "\n",
    "If we expand $\\frac{\\partial MSE}{\\partial \\wv^{[2]}}$ we get the following equation that we need to solve:\n",
    "$$\n",
    "\\frac{\\partial MSE}{\\partial \\wv^{[2]}} = \\frac{\\partial MSE}{\\partial \\av^{[2]}}\\frac{\\partial \\av^{[2]}}{\\partial \\zv^{[2]} }\\frac{\\partial \\zv^{[2]} }{\\partial  \\wv^{[2]} }\n",
    "$$\n",
    "\n",
    "If we compute the derivative of each of the partial derivatives we get the following:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial MSE}{\\partial \\av^{[2]}} &= (\\av^{[2]\\top} - {\\yv}) \\\\\n",
    "\\frac{\\partial \\av^{[2]}}{\\partial \\zv^{[2]} } &= \\mathbf{1} \\\\\n",
    "\\frac{\\partial \\zv^{[2]} }{\\partial  \\wv^{[2]} } &= \\Am^{[1]}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Sadly, we can't always compute $\\frac{\\partial MSE}{\\partial \\wv^{[2]}}$ by simply multiplying all the values for the expanded partial derivative equation. Since we are working with vectors and matrices we need to utilize the dot product and, in turn, rearrange some of the partial derivatives. \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial MSE}{\\partial \\wv^{[2]}} &=  \\frac{\\partial MSE}{\\partial \\av^{[2]}} \\frac{\\partial \\av^{[2]}}{\\partial \\zv^{[2]} } \\frac{\\partial \\zv^{[2]}}{\\partial  \\wv^{[2]}}\\\\\n",
    "\\\\\n",
    "&=  \\big (\\frac{\\partial MSE}{\\partial \\av^{[2]}}^\\top * \\frac{\\partial \\av^{[2]}}{\\partial \\zv^{[2]} } \\big ) \\cdot \\frac{\\partial \\zv^{[2]}}{\\partial  \\wv^{[2]}}^\\top  \\\\\n",
    "\\\\\n",
    "&= \\big( (\\av^{[2]} - {\\yv})^\\top * \\mathbf{1} \\big )  \\cdot  \\Am^{[1]\\top}\n",
    "\\end{align}\n",
    "$$\n",
    "where $\\cdot$ represents the dot product and $*$ represents element-wise multiplication. Notice, that the equation inherently computes the sum of the gradients due to the addition of the dot product. This means we only need to divide by the number of data samples used to compute the gradient to get the average gradient.\n",
    "\n",
    "<!-- *Note: this general equation is needed as we could have the following shape issues when performing element-wise multiplication where we have 2 output neurons, 100 data samples, and 3 hidden units: $\\big[(\\av^{[2]}(2, 100) - \\yv(2, 100))^\\top * \\mathbf{1}(2, 100) \\big ] * \\Am^{[1]}(3, 100)$. Notice, $(2, 100)$ can note be element-wise multiplied with a $(3, 100)$*\n",
    " -->\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46513697",
   "metadata": {
    "id": "46513697"
   },
   "source": [
    "**Bias**\n",
    "\n",
    "If we expand $\\frac{\\partial MSE}{\\partial \\bv^{[2]}}$ we get the following equation that we need to solve:\n",
    "$$\n",
    "\\frac{\\partial MSE}{\\partial \\bv^{[2]}} = \\frac{\\partial MSE}{\\partial \\av^{[2]}}\\frac{\\partial \\av^{[2]}}{\\partial \\zv^{[2]} }\\frac{\\partial \\zv^{[2]} }{\\partial  \\bv^{[2]} }\n",
    "$$\n",
    "Notice, the only NEW partial derivative is $\\frac{\\partial \\zv^{[2]} }{\\partial \\bv^{[2]}}$, the rest we have already seen before. If we compute the partial derivatives we get the following:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\zv^{[2]} }{\\partial  \\bv^{[2]} } &= \\mathbf{1}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Using the same equation format as the output layer weight update, we can compute $\\frac{\\partial MSE}{\\partial \\bv^{[2]}}$ by using the dot product and element-wise multiplication.\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial MSE}{\\partial \\bv^{[2]}} &=  (\\frac{\\partial MSE}{\\partial \\av^{[2]}} * \\frac{\\partial \\av^{[2]}}{\\partial \\zv^{[2]} }) \\cdot \\frac{\\partial \\zv^{[2]}}{\\partial  \\bv^{[2]}}^\\top \\\\\n",
    "&=  \\big ( (\\av^{[2]} - {\\yv})^\\top * \\mathbf{1} \\big ) \\cdot \\mathbf{1}^\\top\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    " Notice, that the equation inherently computes the sum of the gradients due to the addition of the dot product. This means we only need to divide by the number of data samples used to compute the gradient to get the average gradient.\n",
    "\n",
    "<!-- *Note: We could use a similar general equation (as we'll see for the hidden layer bias) that utilizes the dot product for computing $\\frac{\\partial MSE}{\\partial \\bv^{[2]}}$. However, it is never needed to prevent shape mismatch errors as the derivative of $\\frac{\\partial \\zv^{[2]} }{\\partial  \\bv^{[2]} }$ is always 1.* -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0974bc41",
   "metadata": {
    "id": "0974bc41",
    "outputId": "4593915f-52c8-4164-acec-9d9c8653c8d8"
   },
   "outputs": [],
   "source": [
    "print(f\"W['W2'] shape: {W['W2'].shape}\")\n",
    "W['W2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b999dd0",
   "metadata": {
    "id": "2b999dd0",
    "outputId": "6891bdef-6ab4-45ef-e78a-e224cc72e49f"
   },
   "outputs": [],
   "source": [
    "print(f\"b['b2'] shape: {b['b2'].shape}\")\n",
    "b['b2']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591b2d0f",
   "metadata": {
    "id": "591b2d0f"
   },
   "source": [
    "#### TODO 14\n",
    "Complete the TODO by finishing the `get_output_layer_grads()` function to compute the gradients for the output layer weights and biases. **Use the partial derivative solutions and convert them into code where each of the partial derivatives map to the following variables:**\n",
    "- $\\frac{\\partial MSE}{\\partial \\av^{[2]}}$ corresponds to  `delta_mse_A2` \n",
    "- $\\frac{\\partial \\av^{[2]}}{\\partial \\zv^{[2]} }$ corresponds to `delta_A2_Z2`\n",
    "- $ \\frac{\\partial \\zv^{[2]}}{\\partial \\wv^{[2]} }$ corresponds to `delta_Z2_W2`\n",
    "- $\\frac{\\partial \\zv^{[2]} }{\\partial \\bv^{[2]} }$ corresponds to `delta_Z2_B2` \n",
    "\n",
    "*Note: delta corresponds to $\\partial$.*\n",
    "\n",
    "**Output layer weight gradients**\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial MSE}{\\partial \\wv^{[2]}} &=  \\big (\\frac{\\partial MSE}{\\partial \\av^{[2]}}^\\top * \\frac{\\partial \\av^{[2]}}{\\partial \\zv^{[2]} } \\big ) \\cdot \\frac{\\partial \\zv^{[2]}}{\\partial  \\wv^{[2]}}^\\top  \\\\\n",
    "\\\\\n",
    "&= \\big( (\\av^{[2]} - {\\yv})^\\top * \\mathbf{1} \\big )  \\cdot  \\Am^{[1]\\top}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "1. Compute the partial derivative solution $\\frac{\\partial MSE}{\\partial \\wv^{[2]}}$ for the output layer weights $\\Wm^{[2]}$. Store the output into `delta_mse_W2`.\n",
    "\n",
    "2. Compute the average gradient for $\\frac{\\partial MSE}{\\partial \\wv^{[2]}}$ by dividing the number of data samples. Store the output into `W2_avg_grad`.\n",
    "\n",
    "**Output layer bias gradients**\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial MSE}{\\partial \\bv^{[2]}} &=  (\\frac{\\partial MSE}{\\partial \\av^{[2]}} * \\frac{\\partial \\av^{[2]}}{\\partial \\zv^{[2]} }) \\cdot \\frac{\\partial \\zv^{[2]}}{\\partial  \\bv^{[2]}}^\\top \\\\\n",
    "&=  \\big ( (\\av^{[2]} - {\\yv})^\\top * \\mathbf{1} \\big ) \\cdot \\mathbf{1}^\\top\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "3. Compute the partial derivative solution $\\frac{\\partial MSE}{\\partial \\bv^{[2]}}$ for the output layer weights $\\bv^{[2]}$. Store the output into `delta_mse_b2`.\n",
    "\n",
    "4. Compute the average gradient for $\\frac{\\partial MSE}{\\partial \\bv^{[2]}}$ by dividing by the number of data samples. Store the output into `b2_avg_grad`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68068256",
   "metadata": {
    "id": "68068256"
   },
   "outputs": [],
   "source": [
    "def get_output_layer_grads(\n",
    "    y: np.ndarray, \n",
    "    y_hat: np.ndarray, \n",
    "    Zs: dict, \n",
    "    As: dict, \n",
    "    g_output: object, \n",
    "    verbose: bool = False\n",
    "):\n",
    "    \"\"\" Compute gradients for output layer parameters\n",
    "    \n",
    "        Args:\n",
    "            y: NumPy array of labels\n",
    "            \n",
    "            y_hat: NumPy array of predictions\n",
    "            \n",
    "            Zs: Dictionary containing values for the linar combination\n",
    "                of each layer with the weights and inputs\n",
    "            \n",
    "            g_output: Activation function class for output layer\n",
    "            \n",
    "            verbose: If true, all extra information print statements\n",
    "                will be printed.\n",
    "    \n",
    "    \"\"\"\n",
    "    delta_mse_A2 = delta_mse(y, y_hat)\n",
    "    delta_A2_Z2 = g_output.derivative(Zs['Z2'])\n",
    "    delta_Z2_W2 = As['A1']\n",
    "    delta_Z2_b2 = np.ones([1, len(y)])\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\ny shape: {y.shape}\")\n",
    "        print(f\"y_hat shape: {y_hat.shape}\")\n",
    "        print(f\"delta_mse_A2 shape: {delta_mse_A2.shape}\")\n",
    "        print(f\"delta_A2_Z2 shape: {delta_A2_Z2.shape}\")\n",
    "        print(f\"delta_Z2_W2 shape: {delta_Z2_W2.shape}\")\n",
    "        print(f\"delta_Z2_b2 shape: {delta_Z2_b2.shape}\\n\")\n",
    "    \n",
    "    # TODO 14.1\n",
    "    delta_mse_W2 = np.dot(delta_mse_A2.T * delta_A2_Z2, delta_Z2_W2.T)\n",
    "    # TODO 14.2\n",
    "    W2_avg_grad = delta_mse_W2/len(y)\n",
    "    \n",
    "    # TODO 14.3\n",
    "    delta_mse_b2 =  np.dot(delta_mse_A2.T * delta_A2_Z2, delta_Z2_b2.T)\n",
    "    # TODO 14.4\n",
    "    b2_avg_grad = delta_mse_b2/len(y)\n",
    "    \n",
    "    return W2_avg_grad, b2_avg_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31586cc",
   "metadata": {
    "id": "a31586cc"
   },
   "source": [
    "Run the below code to test your implementation of the `get_output_layer_grads()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb47ba6",
   "metadata": {
    "id": "1cb47ba6",
    "outputId": "d35f78e6-632a-4f9b-9017-445d42fc0cac"
   },
   "outputs": [],
   "source": [
    "W2_avg_grad, b2_avg_grad = get_output_layer_grads(\n",
    "    y=y_trn, \n",
    "    y_hat=y_hat, \n",
    "    Zs=Zs, \n",
    "    As=As, \n",
    "    g_output=Linear, \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(f\"Output layer weight grads:{W2_avg_grad}\")\n",
    "print(f\"Output layer bias grads:{b2_avg_grad}\\n\")\n",
    "\n",
    "todo_check([\n",
    "    (W2_avg_grad.shape == (1, 2), \"W2_avg_grad has the wrong shape\"),\n",
    "    (b2_avg_grad.shape == (1, 1), \"b2_avg_grad has the wrong shape\"),\n",
    "    (np.all(np.isclose(W2_avg_grad, np.array([[-0.27083818, -0.26163788]]),rtol=.01)), \"W2_avg_grad has incorrect values\"),\n",
    "    (np.all(np.isclose(b2_avg_grad, np.array([[-0.3407989]]),rtol=.01)), \"b1_avg_grad has incorrect values\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5256aa5c",
   "metadata": {
    "id": "5256aa5c"
   },
   "source": [
    "#### TODO 15 \n",
    "Complete the TODO by updating the weights and biases for the output layer. Use the following update equations for the weights and biases:\n",
    "$$\n",
    "\\Wm^{[2]} = \\wv^{[2]} - \\alpha \\nabla \\frac{\\partial MSE}{\\partial  \\wv^{[2]}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\bv^{[2]} = \\bv^{[2]} - \\alpha \\nabla \\frac{\\partial MSE}{\\partial  \\bv^{[2]}}.\n",
    "$$\n",
    "\n",
    "1. Update the weights stored in `W['W2']` using `alpha` and `W2_avg_grad`. Store the output into `W2_new`.\n",
    "2. Update the weights stored in `b['b2']` using `alpha` and `b2_avg_grad`. Store the output into `b2_new`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d320d70b",
   "metadata": {
    "id": "d320d70b",
    "outputId": "89eea1d4-40d2-4ad1-dae3-2e76b3cd23ac"
   },
   "outputs": [],
   "source": [
    "alpha = .1\n",
    "\n",
    "# TODO 15.1\n",
    "W2_new = W['W2'] - alpha*W2_avg_grad\n",
    "print(f\"Updated output layer weights: {W2_new}\")\n",
    "\n",
    "# TODO 15.2\n",
    "b2_new = b['b2'] - alpha*b2_avg_grad\n",
    "print(f\"Updated output layer biases: {b2_new}\")\n",
    "\n",
    "todo_check([\n",
    "    (np.all(np.isclose(W2_new, np.array([[-0.00660541, -0.22941062]]),rtol=.01)), \"W2_new has incorrect values\"),\n",
    "    (np.all(np.isclose(b2_new, np.array([[1.03407989]]),rtol=.01)), \"b2_new has incorrect values\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2623605",
   "metadata": {
    "id": "b2623605"
   },
   "outputs": [],
   "source": [
    "garbage_collect(['W2_new', 'b2_new', 'W2_avg_grad', 'b2_avg_grad', 'alpha'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad46528c",
   "metadata": {
    "id": "ad46528c"
   },
   "source": [
    "### Hidden layer\n",
    "Now, it's time to propagate the error further into the network. Therefore, we need to now compute the gradients for the hidden layer weights and biases. Thus, we need to compute the partial derivatives $\\frac{\\partial MSE}{\\partial \\Wm^{[1]}}$ and $\\frac{\\partial MSE}{\\partial \\bv^{[1]}}$.\n",
    "\n",
    "\n",
    "This is depicted visually in the below image where we backpropagating through the ENTIRE network to reach the hidden layer.\n",
    "\n",
    "<img src=\"https://live.staticflickr.com/65535/51989492762_456f04dd83_h.jpg\" width=\"400\" height=\"389\" alt=\"signle_nn_hidden_layers_back\">\n",
    "\n",
    "\n",
    "To compute the gradient or partial derivative of the weights $\\Wm^{[1]}$ and biases $\\bv^{[1]}$ we need the following equations:\n",
    "\n",
    "$$\n",
    "MSE = \\frac{1}{2m} (\\av^{[2]} - {\\yv})^2  \\\\ \n",
    "\\av^{[2]} = g(\\zv^{[2]}) \\\\\n",
    "\\zv^{[2]} = \\wv^{[2]} \\Am^{[1]} + \\bv^{[2]}\\\\\n",
    "\\Am^{[1]} = g(\\Zm^{[1]}) \\\\\n",
    "\\Zm^{[1]} = \\Wm^{[1]} \\Xm + \\bv^{[1]}\n",
    "$$\n",
    "where $\\hat{\\yv} = \\av^{[2]\\top}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2486698b",
   "metadata": {
    "id": "2486698b"
   },
   "source": [
    "<hr>\n",
    "\n",
    "**Weights**\n",
    "\n",
    "If we expand $\\frac{\\partial MSE}{\\partial \\Wm^{[1]}}$ we get the following equation:\n",
    "$$\n",
    "\\frac{\\partial MSE}{\\partial \\Wm^{[1]}} = \\frac{\\partial MSE}{\\partial \\av^{[2]}}\\frac{\\partial \\av^{[2]}}{\\partial \\zv^{[2]} }\\frac{\\partial \\zv^{[2]} }{\\partial  \\Am^{[1]} } \\frac{\\partial \\Am^{[1]}}{\\partial \\Zm^{[1]} } \\frac{\\partial \\Zm^{[1]}}{\\partial \\Wm^{[1]} }\n",
    "$$\n",
    "\n",
    "Notice, in order to compute $\\frac{\\partial MSE}{\\partial \\Wm^{[1]}}$ we have to recompute ALL the partial derivatives up to $\\frac{\\partial \\Zm^{[1]}}{\\partial \\Wm^{[1]} }$. This is where we can start to see how backpropagation got its name! If we compute the partial derivatives we get the following:\n",
    "\n",
    "\n",
    "$$\n",
    "\\frac{\\partial MSE}{\\partial \\av^{[2]}} = (\\av^{[2]} - {\\yv}) \\\\\n",
    "\\frac{\\partial \\av^{[2]}}{\\partial \\zv^{[2]} } = \\mathbf{1} \\\\\n",
    "\\frac{\\partial \\zv^{[2]} }{\\partial  \\Am^{[1]} }  = \\wv^{[2]} \\\\\n",
    "\\frac{\\partial \\Am^{[1]}}{\\partial \\Zm^{[1]} } = g( \\Zm^{[1]}) (\\mathbf{1} - g( \\Zm^{[1]})) \\\\\n",
    " \\frac{\\partial \\Zm^{[1]}}{\\partial \\Wm^{[1]} } = \\Xm\n",
    "$$\n",
    "\n",
    "\n",
    "Sadly, we can't compute $\\frac{\\partial MSE}{\\partial \\Wm^{[1]}}$ by simply multiplying all the values for the expanded partial derivative equation. Since we are working with vectors and matrices we need to utilize the dot product and, in turn, rearrange some of the partial derivatives. \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial MSE}{\\partial \\Wm^{[1]}} &= \\frac{\\partial MSE}{\\partial \\av^{[2]}} \\frac{\\partial \\av^{[2]}}{\\partial \\zv^{[2]} }  \\frac{\\partial \\zv^{[2]} }{\\partial \\Am^{[1]} } \\frac{\\partial \\Am^{[1]}}{\\partial \\Zm^{[1]} } \\frac{\\partial \\Zm^{[1]}}{\\partial \\Wm^{[1]} } \\\\\n",
    "\\\\\n",
    " &= \\frac{\\partial \\zv^{[2]} }{\\partial \\Am^{[1]} }^\\top \\cdot \\big(\\frac{\\partial MSE}{\\partial \\av^{[2]}}^\\top * \\frac{\\partial \\av^{[2]}}{\\partial \\zv^{[2]} }\\big) * \\frac{\\partial \\Am^{[1]}}{\\partial \\Zm^{[1]} } \\cdot \\frac{\\partial \\Zm^{[1]}}{\\partial \\Wm^{[1]} }^\\top \\\\\n",
    " \\\\\n",
    "&=   \\Wm^{[2]\\top} \\cdot \\big ( (\\av^{[2]} - {\\yv} )^\\top * \\mathbf{1} \\big )  * \\big( \\Zm^{[1]} *(\\mathbf{1}  - g( \\Zm^{[1]}) \\big )\\cdot \\Xm^\\top\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $\\cdot$ represents the dot product and $*$ represents element-wise multiplication. Notice, that the equation inherently computes the sum of the gradients due to the addition of the dot product. This means we only need to divide by the number of data samples used to compute the gradient to get the average gradient.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34b4ba2",
   "metadata": {
    "id": "b34b4ba2"
   },
   "source": [
    "<hr>\n",
    "\n",
    "**Biases**\n",
    "\n",
    "Thus, if we expand $\\frac{\\partial MSE}{\\partial \\bv^{[1]}}$ we get the following equation:\n",
    "$$\n",
    "\\frac{\\partial MSE}{\\partial \\bv^{[1]}} = \\frac{\\partial MSE}{\\partial \\av^{[2]}}\\frac{\\partial \\av^{[2]}}{\\partial \\zv^{[2]} }\\frac{\\partial \\zv^{[2]} }{\\partial  \\Am^{[1]} } \\frac{\\partial \\Am^{[1]}}{\\partial \\Zm^{[1]} } \\frac{\\partial \\Zm^{[1]}}{\\partial \\bv^{[1]} }\n",
    "$$\n",
    "\n",
    "Notice, the only new partial derivative is $\\frac{\\partial \\Zm^{[1]} }{\\partial \\bv^{[1]}}$, the rest we have already seen before. If we compute the partial derivatives we get the following:\n",
    "\n",
    "\n",
    "$$\n",
    " \\frac{\\partial \\Zm^{[1]}}{\\partial \\bv^{[1]} } = \\mathbf{1}\n",
    "$$\n",
    "\n",
    "Using the same equation format as the hidden layer weight update, we can compute $\\frac{\\partial MSE}{\\partial \\bv^{[1]}}$ by using the dot product and element-wise multiplication. Since we are working with vectors and matrices, we need to utilize the dot product and, in turn, rearrange some of the partial derivatives. \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial MSE}{\\partial \\bv^{[1]}} &= \\frac{\\partial MSE}{\\partial \\av^{[2]}} \\frac{\\partial \\av^{[2]}}{\\partial \\zv^{[2]} }  \\frac{\\partial \\zv^{[2]} }{\\partial \\Am^{[1]} } \\frac{\\partial \\Am^{[1]}}{\\partial \\Zm^{[1]} } \\frac{\\partial \\Zm^{[1]}}{\\partial \\bv^{[1]} } \\\\\n",
    "\\\\\n",
    " &= \\frac{\\partial \\zv^{[2]} }{\\partial \\Am^{[1]} }^\\top \\cdot \\big(\\frac{\\partial MSE}{\\partial \\av^{[2]}} * \\frac{\\partial \\av^{[2]}}{\\partial \\zv^{[2]} }\\big) * \\frac{\\partial \\Am^{[1]}}{\\partial \\Zm^{[1]} } \\cdot \\frac{\\partial \\Zm^{[1]}}{\\partial \\bv^{[1]} }^\\top \\\\\n",
    " \\\\\n",
    "&=   \\Wm^{[2]\\top} \\cdot \\big ( (\\av^{[2]} - {\\yv} )^\\top * \\mathbf{1} \\big )  *\\big( ( \\Zm^{[1]}) (\\mathbf{1} - g( \\Zm^{[1]})) \\big ) \\cdot \\mathbf{1}^\\top\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $\\cdot$ represents the dot product and $*$ represents element-wise multiplication. Notice, that the equation inherently computes the sum of the gradients due to the addition of the dot product. This means we only need to divide by the number of data samples used to compute the gradient to get the average gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798e3c65",
   "metadata": {
    "id": "798e3c65",
    "outputId": "6a1a72fb-bda6-478e-fcb7-b13dfc0d5628"
   },
   "outputs": [],
   "source": [
    "print(f\"W['W1'] shape: {W['W1'].shape}\")\n",
    "W['W1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552ea2f4",
   "metadata": {
    "id": "552ea2f4",
    "outputId": "ebe661d3-7ab0-4a1d-a203-a2413562f7e4"
   },
   "outputs": [],
   "source": [
    "print(f\"b['b1'] shape: {b['b1'].shape}\")\n",
    "b['b1']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ebb761",
   "metadata": {
    "id": "32ebb761"
   },
   "source": [
    "#### TODO 16\n",
    "\n",
    "Each of the partial derivatives map to the following variables where delta corresponds to $\\partial$:\n",
    "- $\\frac{\\partial MSE}{\\partial \\av^{[2]}}$ corresponds to  `delta_mse_A2` \n",
    "- $\\frac{\\partial \\av^{[2]}}{\\partial \\zv^{[2]} }$ corresponds to `delta_A2_Z2`\n",
    "- $\\frac{\\partial \\zv^{[2]} }{\\partial \\Am^{[1]} }$ corresponds to `delta_Z2_A1` \n",
    "- $\\frac{\\partial \\Am^{[1]}}{\\partial \\Zm^{[1]} }$ corresponds to `delta_A1_Z1`\n",
    "- $\\frac{\\partial MSE}{\\partial \\Am^{[1]}}$ corresponds to `delta_mse_A1`\n",
    "- $ \\frac{\\partial \\Zm^{[1]}}{\\partial \\Wm^{[1]} }$ corresponds to `delta_Z1_W1`\n",
    "- $ \\frac{\\partial \\Zm^{[1]}}{\\partial \\bv^{[1]} }$ corresponds to `delta_Z1_b1`\n",
    "\n",
    "*Note: delta corresponds to $\\partial$.*\n",
    "\n",
    "**Output layer activation partial derivative**\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial MSE}{\\partial \\Am^{[1]}} &= \\frac{\\partial \\zv^{[2]} }{\\partial \\Am^{[1]} } \\cdot \\big(\\frac{\\partial MSE}{\\partial \\av^{[2]}} * \\frac{\\partial \\av^{[2]}}{\\partial \\zv^{[2]} }\\big)\n",
    "\\\\\n",
    "&=   \\Wm^{[2]\\top} \\cdot \\big ( (\\av^{[2]} - {\\yv} )^\\top * \\mathbf{1} \\big )\n",
    "\\end{align}\n",
    "$$\n",
    "1. Compute the partial derivative solution $\\frac{\\partial MSE}{\\partial \\Am^{[1]}}$ for the output layer with respect to the  activation $\\Am^{[1]}$. Notice that both weights and biases for the hidden layer use this partial derivative therefore we only need to solve for it once! Store the output into `delta_mse_A1`.\n",
    "\n",
    "\n",
    "**Hidden layer weight gradients**\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial MSE}{\\partial \\Wm^{[1]}} &= \\frac{\\partial MSE}{\\partial \\Am^{[1]}} * \\frac{\\partial \\Am^{[1]}}{\\partial \\Zm^{[1]} } \\cdot \\frac{\\partial \\Zm^{[1]}}{\\partial \\Wm^{[1]} } \\\\\n",
    " \\\\\n",
    "&=   \\Wm^{[2]\\top} \\cdot \\big ( (\\av^{[2]} - {\\yv} )^\\top * \\mathbf{1} \\big )  * \\big( \\Zm^{[1]} *(\\mathbf{1} - g( \\Zm^{[1]}) \\big ) \\cdot \\Xm^\\top\n",
    "\\end{align}\n",
    "$$\n",
    "where $\\Xm = \\Am^{[0]}$\n",
    "\n",
    "2. Compute the partial derivative solution $\\frac{\\partial MSE}{\\partial \\Wm^{[1]}}$ for the output layer weights $\\Wm^{[1]}$. Store the output into `delta_mse_W1`.\n",
    "\n",
    "3. Compute the average gradient for $\\frac{\\partial MSE}{\\partial \\Wm^{[1]}}$ by dividing the number of data samples. Store the output into `W1_avg_grad`.\n",
    "\n",
    "**Hidden layer bias gradients**\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial MSE}{\\partial \\bv^{[1]}} &= \\frac{\\partial MSE}{\\partial \\Am^{[1]}} * \\frac{\\partial \\Am^{[1]}}{\\partial \\Zm^{[1]} } \\cdot \\frac{\\partial \\Zm^{[1]}}{\\partial \\bv^{[1]} }^\\top \\\\\n",
    " \\\\\n",
    "&=  \\Wm^{[2]\\top} \\cdot \\big ( (\\av^{[2]} - {\\yv} )^\\top * \\mathbf{1} \\big )  *\\big( ( \\Zm^{[1]}) (\\mathbf{1} - g( \\Zm^{[1]})) \\big ) \\cdot \\mathbf{1}^\\top\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "4. Compute the partial derivative solution $\\frac{\\partial MSE}{\\partial \\bv^{[1]}}$ for the output layer weights $\\bv^{[1]}$. Store the output into `delta_mse_b1`.\n",
    "\n",
    "5. Compute the average gradient for $\\frac{\\partial MSE}{\\partial \\bv^{[1]}}$ by dividing the number of data samples. Store the output into `b1_avg_grad`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e437b1",
   "metadata": {
    "id": "a4e437b1"
   },
   "outputs": [],
   "source": [
    "def get_hidden_layer_grads(\n",
    "    y: np.ndarray,  \n",
    "    y_hat: np.ndarray,\n",
    "    W: Dict, \n",
    "    Zs: Dict,\n",
    "    As: Dict,\n",
    "    g_hidden: object, \n",
    "    g_output: object, \n",
    "    verbose=False\n",
    "):\n",
    "    \"\"\" Compute gradients for hidden layer parameters\n",
    "    \n",
    "        Args:\n",
    "            y: NumPy array of labels\n",
    "            \n",
    "            y_hat: NumPy array of predictions\n",
    "            \n",
    "            W: Dictionary of weights\n",
    "            \n",
    "            Zs: Dictionary containing values for the linar combination\n",
    "                of each layer with the weights and inputs\n",
    "            \n",
    "            As: Dictionary of neuron activation outputs\n",
    "            \n",
    "            g_hidden: Activation function class for hidden layer\n",
    "            \n",
    "            g_output: Activation function class for output layer\n",
    "            \n",
    "            verbose: If true, all extra information print statements\n",
    "                will be printed.\n",
    "    \n",
    "    \"\"\"\n",
    "    delta_mse_A2 = delta_mse(y, y_hat)\n",
    "    delta_A2_Z2 = g_output.derivative(Zs['Z2'])\n",
    "    delta_Z2_A1 = W['W2']\n",
    "    delta_A1_Z1 = g_hidden.derivative(Zs['Z1'])\n",
    "    delta_Z1_W1 = As['A0']\n",
    "    delta_Z1_b1 = np.ones([1, len(y)])\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\ny shape: {y.shape}\")\n",
    "        print(f\"y_hat shape: {y_hat.shape}\")\n",
    "        print(f\"delta_mse_A2 shape: {delta_mse_A2.shape}\")\n",
    "        print(f\"delta_A2_Z2 shape: {delta_A2_Z2.shape}\")\n",
    "        print(f\"delta_Z2_A1 shape: {delta_Z2_A1.shape}\")\n",
    "        print(f\"delta_A1_Z1 shape: {delta_A1_Z1.shape}\")\n",
    "        print(f\"delta_Z1_W1 shape: {delta_Z1_W1.shape}\")\n",
    "        print(f\"delta_Z1_b1 shape: {delta_Z1_b1.shape}\")\n",
    "    \n",
    "    # TODO 16.1\n",
    "    delta_mse_A1 = np.dot(delta_Z2_A1.T, delta_mse_A2.T * delta_A2_Z2)\n",
    "    if verbose: print(f\"delta_mse_A1 shape: {delta_mse_A1.shape}\\n\")\n",
    "        \n",
    "    # TODO 16.2\n",
    "    delta_mse_W1 = np.dot(delta_mse_A1 * delta_A1_Z1, delta_Z1_W1.T)\n",
    "    \n",
    "    # TODO 16.3\n",
    "    W1_avg_grad = delta_mse_W1/len(y)\n",
    "    # TODO 16.4\n",
    "    delta_mse_b1 = np.dot(delta_mse_A1 * delta_A1_Z1, delta_Z1_b1.T)\n",
    "    # TODO 16.5\n",
    "    b1_avg_grad = delta_mse_b1/len(y)\n",
    "    return W1_avg_grad, b1_avg_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a1f063",
   "metadata": {
    "id": "c8a1f063"
   },
   "source": [
    "Run the below code to test your implementation of the `get_hidden_layer_grads()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd4dd88",
   "metadata": {
    "id": "ffd4dd88",
    "outputId": "61480297-39ca-4114-aa95-c8ee142eeab5"
   },
   "outputs": [],
   "source": [
    "W1_avg_grad, b1_avg_grad = get_hidden_layer_grads(\n",
    "    y=y_trn, \n",
    "    y_hat=y_hat,\n",
    "    W=W,\n",
    "    Zs=Zs, \n",
    "    As=As, \n",
    "    g_hidden=Tanh,\n",
    "    g_output=Linear, \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(f\"Hidden layer weight grads:\\n{W1_avg_grad}\")\n",
    "print(f\"Hidden layer bias grads:\\n{b1_avg_grad}\\n\")\n",
    "\n",
    "todo_check([\n",
    "    (W1_avg_grad.shape == (2, 29), \"W1_avg_grad has the wrong shape\"),\n",
    "    (b1_avg_grad.shape == (2, 1), \"b1_avg_grad has the wrong shape\"),\n",
    "    (np.all(np.isclose(W1_avg_grad[0][:3], np.array([-0.00094709,  0.00000367,  0.00209902]),rtol=.01)), \"W1_avg_grad has incorrect values\"),\n",
    "    (np.all(np.isclose(b1_avg_grad, np.array([[0.00318644],[0.02807532]]),rtol=.01)), \"b1_avg_grad has incorrect values\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c17ca8e",
   "metadata": {
    "id": "0c17ca8e"
   },
   "source": [
    "#### TODO 17 \n",
    "Complete the TODO by updating the weights and biases for the hidden layer. Use the following update equations to for the weights and biases:\n",
    "$$\n",
    "\\Wm^{[1]} = \\Wm^{[1]} - \\alpha \\nabla \\frac{\\partial MSE}{\\partial  \\Wm^{[1]}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\bv^{[1]} = \\bv^{[1]} - \\alpha \\nabla \\frac{\\partial MSE}{\\partial  \\bv^{[1]}}.\n",
    "$$\n",
    "\n",
    "1. Update the weights stored in `W['W1']` using `alpha` and `W1_avg_grad`. Store the output into `W1_new`.\n",
    "\n",
    "2. Update the weights stored in `b['b1']` using `alpha` and `b1_avg_grad`. Store the output into `b1_new`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b635c5",
   "metadata": {
    "id": "48b635c5",
    "outputId": "22faebc6-07b1-456d-c35d-0ee9639cd1e2"
   },
   "outputs": [],
   "source": [
    "alpha = .1\n",
    "# TODO 17.1\n",
    "W1_new = W['W1']-alpha*W1_avg_grad\n",
    "print(f\"Updated hidden layer weights:\\n{W1_new}\")\n",
    "\n",
    "# TODO 17.2\n",
    "b1_new = b['b1']-alpha*b1_avg_grad\n",
    "print(f\"Updated hidden layer biases:\\n{b1_new}\")\n",
    "\n",
    "todo_check([\n",
    "    (np.all(np.isclose(W1_new[0][:3], np.array([0.04890821, 0.215189, 0.10255347]),rtol=.01)), \"W1_new has incorrect values\"), \n",
    "    (np.all(np.isclose(b1_new, np.array([[0.99968136], [0.99719247]]),rtol=.01)), \"b1_new has incorrect values\"), \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008f1860",
   "metadata": {
    "id": "008f1860"
   },
   "outputs": [],
   "source": [
    "garbage_collect(['W1_new', 'b1_new', 'W1_avg_grad', 'b1_avg_grad', 'alpha'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e341d204",
   "metadata": {
    "id": "e341d204"
   },
   "source": [
    "### Putting it all together\n",
    " Now it's time to combine both the forward pass and backward pass code into one class. Thus, we'll need to define a `NeuralNetwork()` class whose `fit()` method perform multiple epochs of mini-batch gradient descent. \n",
    " \n",
    " \n",
    "For each mini-batch we'll make predictions by calling the `forward()` function and then compute the gradients using the `get_output_layer_grads()` and `get_hidden_layer_grads()` functions. Using the gradients for each mini-batch we can then update the weights and biases. We'll perform updates for each mini-batch until we have looped over the entire data. After one epoch (i.e., one pass over the entire data), we can continue running more epochs or end training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9b3966",
   "metadata": {
    "id": "bf9b3966"
   },
   "source": [
    "Below we define our trusty `get_mini_batches()` functions which generates mini-batches for the passed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055889c0",
   "metadata": {
    "id": "055889c0"
   },
   "outputs": [],
   "source": [
    "def get_mini_batches(data_len: int, \n",
    "                     batch_size: int = 32) -> List[np.ndarray]:\n",
    "    \"\"\" Generates mini-batches based on the data indexes\n",
    "        \n",
    "        Args:\n",
    "            data_len: Length of the data or number of data samples \n",
    "                in the data.\n",
    "            \n",
    "            batch_size: Size of each mini-batch where the last mini-batch\n",
    "                might be smaller than the rest if the batch_size does not \n",
    "                evenly divide the data length.\n",
    "    \"\"\"\n",
    "    X_idx = np.arange(data_len)\n",
    "    np.random.shuffle(X_idx)\n",
    "    batches = [X_idx[i:i+batch_size] for i in range(0, data_len, batch_size)]\n",
    "    \n",
    "    return batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9da1225",
   "metadata": {
    "id": "e9da1225"
   },
   "source": [
    "#### TODO 18\n",
    "Complete the TODO by implementing the `NeuralNetwork()` which runs mini-batch gradient descent. You need to simply call the functions we have already defined such as `init_weights()`, `forward()`, `get_output_layer_grads()`, and `get_hidden_layer_grads()`. **Also, be sure to read the documentation for the attributes (class variables) in the `NeuralNetwork` class!**\n",
    "\n",
    "**Weight initialization**\n",
    "\n",
    "1. Call the `init_weights()` function to initialize the weights. Store the outputs into `self.W` and `self.b`.\n",
    "    1. Hint: Remember to pass the required arguments and the `seed` argument. Read the documentation given in the `init_weights()` function to see what each argument corresponds to.\n",
    "\n",
    "**Forward pass**\n",
    "\n",
    "2. Call the `forward()` function to get the predictions for the current mini-batch. Store the outputs into `y_hat`, `Zs`, and `As`. \n",
    "    1. Hint: Remember to pass the required arguments and the current mini-batch of data. Read the documentation given in the `forward()` function to see what each argument corresponds to.\n",
    "\n",
    "**Backwards pass**\n",
    "\n",
    "3. Call the `get_output_layer_grads()` function to get the gradients for the output layer weights and biases. Store the outputs into `W2_avg_grad` and `b2_avg_grad`. \n",
    "    1. Hint: Remember to pass the required arguments and the current mini-batch of labels. Read the documentation given in the `get_output_layer_grads()` function to see what each argument corresponds to.\n",
    "4. Call the `get_hidden_layer_grads()` function to get the gradients for the hidden layer weights and biases. Store the outputs into `W1_avg_grad` and `b1_avg_grad`.\n",
    "    1. Hint: Remember to pass the required arguments and the current mini-batch of labels. Read the documentation given in the `get_hidden_layer_grads()` function to see what each argument corresponds to.\n",
    "5. Update the weights stored in `W['W2']` using `alpha` and `W2_avg_grad`. Store the output into `W['W2']`.\n",
    "6. Update the weights stored in `b['b2']` using `alpha` and `b2_avg_grad`. Store the output into `b['b2']`.\n",
    "7. Update the weights stored in `W['W1']` using `alpha` and `W1_avg_grad`. Store the output into `W['W1']`.\n",
    "2. Update the weights stored in `b['b1']` using `alpha` and `b1_avg_grad`. Store the output into `b['b1']`.\n",
    "\n",
    "**Predicting**\n",
    "\n",
    "9. Call the `forward()` function to get the predictions for the passed data `X`. Store the outputs into `y_hat`, `_`, and `_`. \n",
    "    1. Hint: Remember to pass the required arguments. Read the documentation given in the `forward()` function to see what each argument corresponds to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13943fa0",
   "metadata": {
    "id": "13943fa0"
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork():\n",
    "    def __init__(self,\n",
    "        hidden_neurons: int,\n",
    "        g_hidden: object,\n",
    "        g_output: object,\n",
    "        output_neurons: int = 1,\n",
    "        batch_size: int = 32, \n",
    "        alpha: float = .01, \n",
    "        epochs: int = 1,\n",
    "        verbose: bool = False\n",
    "    ):\n",
    "        \"\"\" A 2 layer neural network which learnins using mini-batch GD \n",
    "            \n",
    "            Attributes:\n",
    "                hidden_neurons: Number of hidden units/neurons.\n",
    "                \n",
    "                output_neurons: Number of output neuorns\n",
    "                \n",
    "                g_hidden: Activation function to be used by the hidden \n",
    "                    units/neurons.\n",
    "                    \n",
    "                g_output: Activation function to be used by the output\n",
    "                    neurons.\n",
    "                \n",
    "                batch_size: Size of each mini-batch.\n",
    "                \n",
    "                alpha: Learning rate to use for gradient descent updates.\n",
    "                \n",
    "                epochs: Number of epochs to use when training.\n",
    "                \n",
    "                verbose: If true all extra infomation print statements will \n",
    "                    be printed.\n",
    "                    \n",
    "                W (dict): Dictionary of weights for each layer\n",
    "                \n",
    "                b (dict): Dictionary of baises for each layer\n",
    "                \n",
    "                epoch_losses (list): Tracks average training loss per epoch\n",
    "                \n",
    "                vld_epoch_losses (list): Tracks average validation loss per\n",
    "                    epoch.\n",
    "        \"\"\"\n",
    "        self.hidden_neurons = hidden_neurons\n",
    "        self.output_neurons = output_neurons\n",
    "        self.g_hidden = g_hidden\n",
    "        self.g_output = g_output\n",
    "        self.batch_size = batch_size\n",
    "        self.alpha = alpha\n",
    "        self.epochs = epochs\n",
    "        self.verbose = verbose\n",
    "        self.W = None\n",
    "        self.b = None\n",
    "        self.epoch_losses = None\n",
    "        self.vld_epoch_losses = None\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        X: np.ndarray, \n",
    "        y: np.ndarray,\n",
    "        *,\n",
    "        X_vld: np.ndarray = None, \n",
    "        y_vld:np.ndarray = None,\n",
    "        seed: int = 0,\n",
    "    ): \n",
    "        \"\"\" Training method for a 2 layer neural network \n",
    "        \n",
    "            Args:\n",
    "                X: Data used for training neural network\n",
    "                \n",
    "                y: Labels corresponding to data used for training the\n",
    "                    neural network.\n",
    "                \n",
    "                X_vld: Validation data for assessing network performance.\n",
    "                \n",
    "                y_vld: Validation lables which correspond to validation\n",
    "                    data.\n",
    "                    \n",
    "                seed: Used to make neural network training reproducible.\n",
    "                    Controls the selection of mini-batches and initial\n",
    "                    weight values.\n",
    "        \"\"\"\n",
    "        np.random.seed()\n",
    "        m = len(X)\n",
    "        self.epoch_losses = []\n",
    "        self.vld_epoch_losses = []\n",
    "\n",
    "        # Initialize weights and biases\n",
    "        # TODO 18.1\n",
    "        self.W, self.b = init_weights(X.shape[1], self.hidden_neurons, self.output_neurons, seed = seed)\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"self.W['W1'] shape: {self.W['W1'].shape}\")\n",
    "            print(f\"self.b['b1'] shape: {self.b['b1'].shape}\")\n",
    "            print(f\"self.W['W2'] shape: {self.W['W2'].shape}\")\n",
    "            print(f\"self.b['b2'] shape: {self.b['b2'].shape}\")\n",
    "        \n",
    "        # Iterate epochs\n",
    "        for e in range(self.epochs):\n",
    "            print(f\"Epoch: {e+1}\")\n",
    "            batches = get_mini_batches(data_len=m, batch_size=self.batch_size)\n",
    "            epoch_sse = 0\n",
    "            # Iterate mini-batch\n",
    "            for mb in batches:\n",
    "                # Forward pass to get predictions\n",
    "                # TODO 18.2\n",
    "                y_hat, Zs, As = forward(X[mb], self.W, self.b, self.g_hidden, self.g_output)\n",
    "\n",
    "                # Backward pass to update weights and biases\n",
    "                # TODO 18.3\n",
    "                W2_avg_grad, b2_avg_grad = get_output_layer_grads(y[mb], y_hat, Zs, As, self.g_output)\n",
    "            \n",
    "                # TODO 18.4\n",
    "                W1_avg_grad, b1_avg_grad = get_hidden_layer_grads(y[mb], y_hat, self.W, Zs, As, self.g_hidden, self.g_output)\n",
    "                                        \n",
    "                # Update output layer's weights and biases\n",
    "                # TODO 18.5\n",
    "                self.W['W2'] = self.W['W2'] - self.alpha*W2_avg_grad\n",
    "                # TODO 18.6\n",
    "                self.b['b2'] = self.b['b2'] - self.alpha*b2_avg_grad\n",
    "                \n",
    "                # Update hidden layer's weights and biases\n",
    "                # TODO 18.7\n",
    "                self.W['W1'] = self.W['W1'] - self.alpha*W1_avg_grad\n",
    "                # TODO 18.8\n",
    "                self.b['b1'] = self.b['b1'] - self.alpha*b1_avg_grad\n",
    "\n",
    "                # Compute squared error for current mini-batch\n",
    "                batch_sse = sse(y[mb], y_hat)\n",
    "                epoch_sse += batch_sse\n",
    "\n",
    "            # Compute MSE for entire dataset\n",
    "            epoch_mse = epoch_sse / m\n",
    "            self.epoch_losses.append(epoch_mse)\n",
    "            print(f\"\\t Train MSE: {epoch_mse}\")\n",
    "             \n",
    "            # Compute validation MSE if data was passed\n",
    "            if X_vld is not None and y_vld is not None:\n",
    "                vld_epoch_mse = self.check_valid_scores(X_vld, y_vld)\n",
    "                self.vld_epoch_losses.append(vld_epoch_mse)\n",
    "                print(f\"\\t Valid MSE: {vld_epoch_mse}\")\n",
    "    \n",
    "    def check_valid_scores(self, X_vld, y_vld):\n",
    "        y_hat, _, _ = forward(\n",
    "            X=X_vld,\n",
    "            W=self.W,\n",
    "            b=self.b,\n",
    "            g_hidden=self.g_hidden,\n",
    "            g_output=self.g_output,\n",
    "        )\n",
    "        vld_mse = mse(y_vld, y_hat)\n",
    "        return vld_mse\n",
    "    \n",
    "    def predict(self, X: np.ndarray):\n",
    "        # TODO 18.9\n",
    "        y_hat, _, _  = forward(X, self.W, self.b, self.g_hidden, self.g_output)\n",
    "        \n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23c0d3d",
   "metadata": {
    "id": "c23c0d3d"
   },
   "source": [
    "Run the `TEST_neural_network()` function code to test your implementation of the `NeuralNetwork` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d90e23a",
   "metadata": {
    "id": "2d90e23a",
    "outputId": "6da142d1-1857-4179-dcaf-55f412b4c377"
   },
   "outputs": [],
   "source": [
    "def TEST_neural_network():\n",
    "    \n",
    "    def nonlinear_data(m=100):\n",
    "        rng = np.random.RandomState(42)\n",
    "        X = np.sort(6 * rng.rand(m, 1) - 3, axis=0)\n",
    "\n",
    "        y = (2 + .5* X**2 + X) + rng.rand(m, 1)\n",
    "\n",
    "        return X, y.reshape(-1, 1)\n",
    "    \n",
    "    nn = NeuralNetwork(\n",
    "        alpha=.1,\n",
    "        hidden_neurons=20,\n",
    "        g_hidden=Sigmoid,\n",
    "        g_output=Linear,\n",
    "        batch_size=64, \n",
    "        epochs=500\n",
    "    )\n",
    "    \n",
    "    X, y = nonlinear_data()\n",
    "    nn.fit(X, y)\n",
    "    \n",
    "    y_hat = nn.predict(X)\n",
    "    mse_ = mse(y=y, y_hat=y_hat)\n",
    "\n",
    "    plt.plot(y, '.')\n",
    "    plt.plot(y_hat, '.')\n",
    "    plt.title(f\"Final MSE {mse_}\")\n",
    "    \n",
    "    todo_check([\n",
    "        (np.isclose(mse_, .165, rtol=.1), \"mse_ value is incorrect\")\n",
    "    ])\n",
    "    \n",
    "TEST_neural_network()\n",
    "garbage_collect(['TEST_neural_network'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675acea1",
   "metadata": {
    "id": "675acea1"
   },
   "source": [
    "# Non-linear Regression\n",
    "\n",
    "Now it's time to apply our neural network implementation to the Forest Fire dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebb5bd3",
   "metadata": {
    "id": "2ebb5bd3"
   },
   "source": [
    "## Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fbe597",
   "metadata": {
    "id": "18fbe597"
   },
   "source": [
    "#### TODO 19\n",
    "Let's try training our `NeuralNetwork` class. The goal for the this TODO will be to get your validation MSE below 2. To do so, you'll have to manually tune the hyper-parameters such as the number of hidden units `hidden_neurons`, which activation function you'll use for `g_hidden` (`Sigmoid` or `Tanh`), the learning rate `alpha`, the size of the mini-batches `batch_size`, and the number of epochs `epochs`.\n",
    "\n",
    "1. Call the `data_prep()` so that it will return the Forest Fire regression data formatted as NumPy arrays. Do so by passing the arguments which correspond to the following:\n",
    "    1. Pass any required arguments (i.e., arguments with no default values).\n",
    "    1. Return all data as NumPy arrays.\n",
    "\n",
    "\n",
    "2. Create an instance of the `NeuralNetwork` class. Store the output into `nn`. In order to pass the TODO check you will need to tune the hyper-parameters (i.e., arguments passed to the  `NeuralNetwork` class) such that you get a validation MSE score below 2. Thus, you need to determine which arguments to pass to the `NeuralNetwork` class. The only argument that should remain static is the `g_output` argument. Recall, this will always need to be set to the `Linear` class for regression problems.\n",
    "    1. Hint: Use the learning curve plot that is plotted at the end of the training to help select how many epochs you need. Refer to this [post](https://machinelearningmastery.com/learning-curves-for-diagnosing-machine-learning-model-performance/) or last weeks lab on how to interpret the training and validation learning curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b492bae",
   "metadata": {
    "id": "3b492bae",
    "outputId": "b4cc8636-656f-4ed0-d811-b29b207b8a19",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO 19.1\n",
    "data = data_prep(df=forestfire_df,label_name='area', return_array=True)\n",
    "X_trn, y_trn, X_vld, y_vld, X_tst, y_tst = data\n",
    "\n",
    "# TODO 19.2\n",
    "nn = NeuralNetwork(\n",
    "        alpha=.01,\n",
    "        hidden_neurons=18,\n",
    "        g_hidden=Sigmoid,\n",
    "        g_output=Sigmoid,\n",
    "        batch_size=64, \n",
    "        epochs=300\n",
    "    )\n",
    "\n",
    "nn.fit(X_trn, y_trn, X_vld=X_vld, y_vld=y_vld)\n",
    "vld_mse = mse(y_vld, nn.predict(X_vld))\n",
    "\n",
    "print(\"-\"*50)\n",
    "plt.plot(nn.epoch_losses, label='Train loss')\n",
    "plt.plot(nn.vld_epoch_losses, label='Valid loss')\n",
    "plt.title(\"Learning Curve\")\n",
    "plt.ylabel(\"Average MSE\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"-\"*50)\n",
    "print(f\"Valid MSE score: {vld_mse}\")\n",
    "todo_check([\n",
    "    (vld_mse < 1.97, \"vld_mse was not less than 1.97\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa33aa6",
   "metadata": {
    "id": "faa33aa6",
    "outputId": "972a8ac8-ef95-4d37-8aa3-ce18a8e7ae37"
   },
   "outputs": [],
   "source": [
    "y_hat_trn = nn.predict(X_trn)\n",
    "\n",
    "trn_sse, trn_mse, trn_rmse = analyze(y_trn, y_hat_trn,\n",
    "                                        title=\"Training Predictions Log Transform\",\n",
    "                                        dataset=\"Training\",\n",
    "                                        xlabel=\"Data Sample Index\",\n",
    "                                        ylabel=\"Predicted Log Area\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d706f009",
   "metadata": {
    "id": "d706f009"
   },
   "source": [
    "### Validation evaluation \n",
    "\n",
    "Use the below code to visualize the results for your validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafec6cc",
   "metadata": {
    "id": "bafec6cc",
    "outputId": "d44c63ef-fb18-4d5d-af8b-b0951c923698",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_hat_vld = nn.predict(X_vld)\n",
    "\n",
    "analyze(y_vld, y_hat_vld,\n",
    "        title=\"Validation Predictions Log Transform\",\n",
    "        dataset=\"Validation\",\n",
    "        xlabel=\"Data Sample Index\",\n",
    "        ylabel=\"Log Area\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef935c9f",
   "metadata": {
    "id": "ef935c9f"
   },
   "source": [
    "### Testing evaluation\n",
    "\n",
    "Use the below code to visualize the results for your testing data. Depending on your hyper-parameter fine tuning, you should notice that, for the most part, performance is on par or slightly worse than the linear regression and polynomial regression algorithms which achieved around ~1.4 RMSE. \n",
    "\n",
    "Once again, this is most likely because we need more data and possibly better features to solve this problem. Keep in mind, neural networks aren't a golden bullet. Meaning, they aren't the best algorithm for every problem (hence recall back to the [no free lunch theorem](https://machinelearningmastery.com/no-free-lunch-theorem-for-machine-learning/)). As we'll see in the near future, deep learning can be quite powerful but even it too has its limitations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "297d7736",
   "metadata": {
    "id": "297d7736",
    "outputId": "ce886b2c-1925-425b-bc09-eeacc1dd2032"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m y_hat_tst \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241m.\u001b[39mpredict(X_tst)\n\u001b[0;32m      3\u001b[0m analyze(y_tst, y_hat_tst,\n\u001b[0;32m      4\u001b[0m         title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Predictions Log Transform\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      5\u001b[0m         dataset\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      6\u001b[0m         xlabel\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData Sample Index\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      7\u001b[0m         ylabel\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLog Area\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "y_hat_tst = nn.predict(X_tst)\n",
    "\n",
    "analyze(y_tst, y_hat_tst,\n",
    "        title=\"Test Predictions Log Transform\",\n",
    "        dataset=\"Test\",\n",
    "        xlabel=\"Data Sample Index\",\n",
    "        ylabel=\"Log Area\");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e01422b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "1626px",
    "left": "566px",
    "top": "111.133px",
    "width": "189.733px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
